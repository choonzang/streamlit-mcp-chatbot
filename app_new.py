import streamlit as st
import os
import json
import time
import asyncio
from dotenv import load_dotenv
from typing import List, Dict, Any, AsyncGenerator
from datetime import datetime
from pathlib import Path

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain.agents import AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser

from langgraph.prebuilt import create_react_agent

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë¡œë“œ ---
load_dotenv()

# -----------------------------------------------------------------------------
# ì‹¤ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³ , ìœ„ì˜ Mock ê°ì²´ë“¤ì€ ì‚­ì œí•˜ì„¸ìš”.
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
# -----------------------------------------------------------------------------

# --- ìƒìˆ˜ ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
MCP_CONFIG_FILE = "mcp.json"
HISTORY_DIR = Path("chat_histories")
HISTORY_DIR.mkdir(exist_ok=True) # ëŒ€í™” ê¸°ë¡ ì €ì¥ í´ë” ìƒì„±

# ì „ì—­ ë³€ìˆ˜ ëŒ€ì‹  st.session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ LLM ëª¨ë¸ ì„ íƒì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
llm_options = {
    "OpenAI":['o4-mini','o3','o3-mini','o1','o1-mini','gpt-4o','gpt-4.1'],
    "Gemini":['gemini-2.0-flash-001','gemini-2.5-flash','gemini-1.5-flash'],
    "Claude":['claude-3-7-sonnet-20250219', 'claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022','claude-3-5-sonnet-20240620','claude-sonnet-4-20250514']
}

# --- í—¬í¼ í•¨ìˆ˜ ---

def run_async(func):
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ Streamlitì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í—¬í¼"""
    return asyncio.run(func)

def generate_filename_with_timestamp(prefix="chat_", extension="json"):
    """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    now = datetime.now()
    timestamp_str = now.strftime("%Y%m%d_%H%M%S")
    if prefix:
        filename = f"{prefix}{timestamp_str}.{extension}"
    else:
        filename = f"{timestamp_str}.{extension}"
    return filename

# @st.cache_resource ëŒ€ì‹  st.session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
def get_llm():
    """LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    category = st.session_state.get('selected_llm_category', 'OpenAI')
    model_name = st.session_state.get('selected_llm_model', 'o4-mini')
    
    if category == 'Claude':
        llm = ChatAnthropic(model=model_name, temperature=0, max_tokens=4096)
    elif category == 'OpenAI':
        llm = ChatOpenAI(model=model_name, max_tokens=8000)
    elif category == 'Gemini':
        llm = ChatGoogleGenerativeAI(model=model_name)
    else: # ê¸°ë³¸ê°’
        llm = ChatOpenAI(model="o4-mini", temperature=0,  max_tokens=8000)
    return llm

# @st.cache_data
def load_mcp_config():
    """mcp.json ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    with open(MCP_CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_mcp_config(config):
    """MCP ì„œë²„ ì„¤ì •ì„ mcp.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(MCP_CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

# --- â˜…â˜…â˜…â˜…â˜… í•µì‹¬ ë¡œì§ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ë° ë‹¨ì¼ ì—ì´ì „íŠ¸ ëª¨ë¸ë¡œ ëŒ€í­ ìˆ˜ì •) â˜…â˜…â˜…â˜…â˜… ---

async def select_mcp_servers(query: str, servers_config: Dict) -> List[str]:
    """ì‚¬ìš©ì ì§ˆì˜ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©í•  MCP ì„œë²„ë¥¼ LLMì„ í†µí•´ ì„ íƒí•©ë‹ˆë‹¤."""
    llm = get_llm()
    
    system_prompt = "You are a helpful assistant that selects the most relevant tools for a given user query."
    prompt_template = """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ê·¸ 'description'ì„ ë³´ê³  ì„ íƒí•´ì£¼ì„¸ìš”.
    ì„ íƒëœ ë„êµ¬ì˜ ì´ë¦„(í‚¤ ê°’)ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ëª©ë¡ìœ¼ë¡œë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. (ì˜ˆ: weather,Home Assistant)
    ë§Œì•½ ì í•©í•œ ë„êµ¬ê°€ ì—†ë‹¤ë©´ 'None'ì´ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”.

    [ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡]
    {tools_description}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_query}

    [ì„ íƒëœ ë„êµ¬ ëª©ë¡]
    """
    
    descriptions = "\n".join([f"- {name}: {config['description']}" for name, config in servers_config.items()])
    
    prompt = ChatPromptTemplate.from_template(prompt_template).format(
        tools_description=descriptions,
        user_query=query
    )
    
    response = await llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=prompt)])
    selected = [s.strip() for s in response.content.split(',') if s.strip() and s.strip().lower() != 'none']
    return selected

async def process_query(query: str, chat_history: List) -> AsyncGenerator[str, None]:
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ ì„œë²„ ì„ íƒ, ì—ì´ì „íŠ¸ ìƒì„±, ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì˜ ì „ì²´ ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    mcp_config = load_mcp_config()["mcpServers"]
    llm = get_llm()
    messages = chat_history + [HumanMessage(content=query)]

    # 1. MCP ì„œë²„ ë¼ìš°íŒ…
    st.write("`1. MCP ì„œë²„ ë¼ìš°íŒ… ì¤‘...`")
    selected_server_names = await select_mcp_servers(query, mcp_config)

    # ìš”ì²­ 1: ì—°ê²°í•  MCP ì„œë²„ê°€ ì—†ì„ ê²½ìš°, LLMìœ¼ë¡œ ì§ì ‘ ì§ˆì˜ (ìŠ¤íŠ¸ë¦¬ë°)
    if not selected_server_names:
        st.info("âœ… ì í•©í•œ ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLMì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤.")
        async for chunk in llm.astream(messages):
            yield chunk.content
        return

    # 2. ì„ íƒëœ ì„œë²„ì—ì„œ ëª¨ë“  ë„êµ¬ë¥¼ ìˆ˜ì§‘
    st.write(f"`2. ì„ íƒëœ ì„œë²„: {', '.join(selected_server_names)}`")
    all_tools = []
    
    st.write("`3. ì„ íƒëœ MCP ì„œë²„ ì„¸ì…˜ ë° ë„êµ¬ ë¡œë”© ì¤‘...`")
    
    # ë¹„ë™ê¸° ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ë” ë¹ ë¥¸ ë¡œë”©
    async def load_tools_from_server(name, config):
        try:
            conn_type = config.get("transport")
            if conn_type == "stdio":
                server_params = StdioServerParameters(command=config.get("command"), args=config.get("args", []))
                async with stdio_client(server_params) as (read, write):
                    async with ClientSession(read, write) as session:
                        await session.initialize()
                        return await load_mcp_tools(session), name
            elif conn_type == "sse":
                url = config.get("url")
                headers = config.get("headers", {})
                async with sse_client(url, headers=headers) as (read, write):
                    async with ClientSession(read, write) as session:
                        await session.initialize()
                        return await load_mcp_tools(session), name
            else:
                st.warning(f"âš ï¸ '{name}' ì„œë²„ì˜ ì—°ê²° íƒ€ì… ('{conn_type}')ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return [], name
        except Exception as e:
            st.error(f"âŒ '{name}' MCP ì„œë²„ ì—°ê²° ë˜ëŠ” ë„êµ¬ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return [], name

    tasks = [load_tools_from_server(name, mcp_config[name]) for name in selected_server_names]
    results = await asyncio.gather(*tasks)

    for tools, name in results:
        if tools:
            all_tools.extend(tools)
            st.success(f"âœ… '{name}' ì„œë²„ ì—°ê²° ë° ë„êµ¬ ë¡œë“œ ì„±ê³µ: `{[tool.name for tool in tools]}`")
        else:
            st.warning(f"âœ… '{name}' ì„œë²„ì— ì—°ê²°í–ˆìœ¼ë‚˜, ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # 3. ìˆ˜ì§‘ëœ ë„êµ¬ê°€ ì—†ìœ¼ë©´ LLMìœ¼ë¡œ ì§ì ‘ ë‹µë³€ (ìŠ¤íŠ¸ë¦¬ë°)
    if not all_tools:
        st.info("âœ… ë„êµ¬ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLMì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤.")
        async for chunk in llm.astream(messages):
            yield chunk.content
        return

    # 4. ìˆ˜ì§‘ëœ ëª¨ë“  ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰
    st.write(f"`4. ì´ {len(all_tools)}ê°œ ë„êµ¬ë¡œ ë‹¨ì¼ ì—ì´ì „íŠ¸ ìƒì„± ë° ì§ˆì˜ ì‹¤í–‰...`")
    
    try:
        agent = create_react_agent(llm, all_tools)
        agent_input = {"messages": messages}
        
        # â˜…â˜…â˜…â˜…â˜… í•´ê²° ë°©ì•ˆ: astream_eventsë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì˜ ìµœì¢… ì‘ë‹µ í† í°ë§Œ í•„í„°ë§ â˜…â˜…â˜…â˜…â˜…
        # version="v1"ì€ LangChainì˜ í‘œì¤€ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
        async for event in agent.astream_events(agent_input, version="v2"):
            
            kind = event["event"]
            
            # ì—ì´ì „íŠ¸ ë‚´ì˜ Chat Modelì—ì„œ í† í° ìŠ¤íŠ¸ë¦¼ì´ ë°œìƒí•  ë•Œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            # ì´ê²ƒì´ ë°”ë¡œ ì‚¬ìš©ìê°€ ë³´ê²Œ ë  ìµœì¢… ì‘ë‹µì˜ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ì…ë‹ˆë‹¤.
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                # ë¹„ì–´ìˆì§€ ì•Šì€ í† í°ë§Œ yieldí•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
                if content:
                    yield content

    except Exception as e:
        error_message = f"âŒ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        st.error(error_message)
        yield error_message


# --- Streamlit UI êµ¬ì„± ---

st.set_page_config(page_title="MCP Client on Streamlit", layout="wide")
st.title("ğŸ¤– MCP(Model Context Protocol) í´ë¼ì´ì–¸íŠ¸")

# 1. ì¸ì¦ ì²˜ë¦¬
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if st.button("ë¡œê·¸ì¸"):
        if password == os.getenv("APP_PASSWORD"):
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# 2. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (ì¸ì¦ í›„)
# ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.header("ë©”ë‰´")
    if st.button("ë¡œê·¸ì•„ì›ƒ"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()
    
    def start_new_chat():
        st.session_state.messages = []
        st.session_state.current_chat_file = None

    def auto_save_chat():
        if st.session_state.get("current_chat_file") and st.session_state.get("messages"):
            save_path = HISTORY_DIR / st.session_state.current_chat_file
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)

    def load_chat(filename: str):
        load_path = HISTORY_DIR / filename
        with open(load_path, "r", encoding="utf-8") as f:
            st.session_state.messages = json.load(f)
        st.session_state.current_chat_file = filename

    def delete_chat(filename: str):
        if st.session_state.get("current_chat_file") == filename:
            start_new_chat()
        
        file_to_delete = HISTORY_DIR / filename
        if file_to_delete.exists():
            file_to_delete.unlink()
            st.toast(f"'{filename}'ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    st.button("ìƒˆë¡œìš´ ì±„íŒ… ì—´ê¸°", on_click=start_new_chat, use_container_width=True)
    st.divider()

    st.header("LLM ê´€ë¦¬")
    
    selected_category = st.selectbox(
        "LLMë¥¼ ì„ íƒí•˜ì„¸ìš”:", 
        list(llm_options.keys()), 
        key='selected_llm_category'
    )
    
    selected_item = st.selectbox(
        f"{selected_category} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”:", 
        llm_options[selected_category],
        key='selected_llm_model'
    )

    st.divider()
    st.header("MCP ì„œë²„ ê´€ë¦¬")
    mcp_config = load_mcp_config()
    with st.expander("ì„œë²„ ëª©ë¡ ë³´ê¸°/ê´€ë¦¬"):
        st.json(mcp_config)
        servers = list(mcp_config["mcpServers"].keys())
        server_to_delete = st.selectbox("ì‚­ì œí•  ì„œë²„ ì„ íƒ", [""] + servers)
        if st.button("ì„ íƒëœ ì„œë²„ ì‚­ì œ", type="primary"):
            if server_to_delete and server_to_delete in mcp_config["mcpServers"]:
                del mcp_config["mcpServers"][server_to_delete]
                save_mcp_config(mcp_config)
                st.success(f"'{server_to_delete}' ì„œë²„ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                time.sleep(1); st.rerun()
        st.markdown("---")
        st.write("**ìƒˆ ì„œë²„ ì¶”ê°€**")
        new_server_name = st.text_input("ìƒˆ ì„œë²„ ì´ë¦„")
        new_server_config_str = st.text_area("ìƒˆ ì„œë²„ JSON ì„¤ì •", height=200, placeholder='{\n  "description": "...",\n ...}')
        if st.button("ìƒˆ ì„œë²„ ì¶”ê°€"):
            if new_server_name and new_server_config_str:
                try:
                    new_config = json.loads(new_server_config_str)
                    mcp_config["mcpServers"][new_server_name] = new_config
                    save_mcp_config(mcp_config)
                    st.success(f"'{new_server_name}' ì„œë²„ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    time.sleep(1); st.rerun()
                except json.JSONDecodeError: st.error("ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
            else: st.warning("ì„œë²„ ì´ë¦„ê³¼ ì„¤ì •ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()

    st.header("ì €ì¥ëœ ëŒ€í™”")
    saved_chats = sorted([f for f in os.listdir(HISTORY_DIR) if f.endswith(".json")], reverse=True)
    
    if not saved_chats:
        st.write("ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    for filename in saved_chats:
        col1, col2 = st.columns([0.85, 0.15])
        with col1:
            is_active_chat = st.session_state.get("current_chat_file") == filename
            button_type = "primary" if is_active_chat else "secondary"
            if st.button(filename, key=f"load_{filename}", use_container_width=True, type=button_type):
                if not is_active_chat:
                    load_chat(filename)
                    st.rerun()
        with col2:
            if st.button("X", key=f"delete_{filename}", use_container_width=True, help=f"{filename} ì‚­ì œ"):
                delete_chat(filename)
                st.rerun()

# --- ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---

if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_chat_file" not in st.session_state:
    st.session_state.current_chat_file = None

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- â˜…â˜…â˜…â˜…â˜… ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ë° ì €ì¥ ë¡œì§ìœ¼ë¡œ ë³€ê²½) â˜…â˜…â˜…â˜…â˜… ---
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜ ê·¸ë¦¬ê³  ê±°ì‹¤ ë¶ˆ ì¼œì¤˜)"):
    if not st.session_state.get("current_chat_file"):
        st.session_state.current_chat_file = generate_filename_with_timestamp()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            history = [
                HumanMessage(content=m['content']) if m['role'] == 'user' else AIMessage(content=m['content'])
                for m in st.session_state.messages[:-1]
            ]
            
            # st.write_streamì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ í‘œì‹œí•˜ê³ , ìµœì¢… ì‘ë‹µì„ response ë³€ìˆ˜ì— ì €ì¥
            response_generator = process_query(prompt, history)
            response = st.write_stream(response_generator)

        st.badge(f"Answer by {st.session_state.get('selected_llm_model', 'N/A')}", icon=":material/check:", color="green")
    
    # ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œëœ í›„, ìµœì¢… ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})
    
    # ëŒ€í™” í„´ì´ ëë‚œ í›„, ìë™ ì €ì¥
    auto_save_chat()

    # st.rerun()ì„ í˜¸ì¶œí•˜ì§€ ì•Šì•„ë„ UIê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì—…ë°ì´íŠ¸ë¨