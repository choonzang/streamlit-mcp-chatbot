import streamlit as st
from streamlit_local_storage import LocalStorage

import os
import json
import time
import asyncio
import shutil
from dotenv import load_dotenv
from typing import List, Dict, AsyncGenerator
from datetime import datetime
from pathlib import Path
import tiktoken

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain.agents import AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser

from langgraph.prebuilt import create_react_agent

from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë¡œë“œ ---
load_dotenv()

# -----------------------------------------------------------------------------
# ì‹¤ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
# -----------------------------------------------------------------------------

# --- ìƒìˆ˜ ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
BASE_HISTORY_DIR = Path("chat_histories")
BASE_HISTORY_DIR.mkdir(exist_ok=True) # ê¸°ë³¸ ëŒ€í™” ê¸°ë¡ ì €ì¥ í´ë” ìƒì„±

global selected_category
global selected_item
selected_category = None
selected_item = None
llm_options = {
    "OpenAI":['gpt-5.1-2025-11-13','gpt-5-2025-08-07','gpt-4.1-nano','gpt-4.1-mini','gpt-4.1','gpt-4o','o4-mini','o3','o3-mini','o1','o1-mini'],
    "Gemini":['gemini-2.0-flash-001','gemini-2.5-flash','gemini-1.5-flash'],
    "Claude":['claude-3-7-sonnet-20250219', 'claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022','claude-3-5-sonnet-20240620','claude-sonnet-4-20250514']
}
#'claude-opus-4-20250514'

# --- í—¬í¼ í•¨ìˆ˜ ---
def get_user_history_dir() -> Path:
    """ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì˜ ëŒ€í™” ê¸°ë¡ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if st.session_state.get("authenticated"):
        username = st.session_state.get("username", "default")
        user_dir = BASE_HISTORY_DIR / username
        user_dir.mkdir(exist_ok=True)
        return user_dir
    return BASE_HISTORY_DIR

def get_mcp_config_file() -> str:
    """ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì˜ mcp.json íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if st.session_state.get("authenticated"):
        username = st.session_state.get("username", "default")
        return f"mcp_{username}.json"
    return "mcp.json"

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def generate_filename_with_timestamp(prefix="chat_", extension="json"):
    """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    now = datetime.now()
    timestamp_str = now.strftime("%Y%m%d_%H%M%S")
    if prefix:
        filename = f"{prefix}{timestamp_str}.{extension}"
    else:
        filename = f"{timestamp_str}.{extension}"
    return filename

def get_llm():
    """LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    if selected_category == 'Claude':
        llm = ChatAnthropic(model=selected_item, temperature=0, max_tokens=4096)
    elif selected_category == 'OpenAI':
        llm = ChatOpenAI(model=selected_item, max_tokens=8000)
    elif selected_category == 'Gemini':
        llm = ChatGoogleGenerativeAI(model=selected_item)
    else:
        llm = ChatOpenAI(model="o4-mini", temperature=0,  max_tokens=8000)
    return llm

def load_mcp_config():
    """ì‚¬ìš©ìë³„ mcp.json ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    config_file = get_mcp_config_file()
    if not os.path.exists(config_file):
        # ì‚¬ìš©ìë³„ ì„¤ì • íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ mcp.jsonìœ¼ë¡œ ìƒì„±
        if os.path.exists("mcp.json"):
            shutil.copy("mcp.json", config_file)
            st.toast(f"'{config_file}'ì´(ê°€) ì—†ì–´ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        else:
            # ê¸°ë³¸ íŒŒì¼ë„ ì—†ìœ¼ë©´ ë¹ˆ ì„¤ì •ìœ¼ë¡œ ìƒì„±
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump({"mcpServers": {}}, f, indent=2, ensure_ascii=False)
            st.toast(f"'{config_file}'ì´(ê°€) ì—†ì–´ ë¹ˆ ì„¤ì • íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

    with open(config_file, "r", encoding="utf-8") as f:
        return json.load(f)

def save_mcp_config(config):
    """MCP ì„œë²„ ì„¤ì •ì„ ì‚¬ìš©ìë³„ mcp.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(get_mcp_config_file(), 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

def rename_chat(old_filename: str, new_filename_base: str):
    """ëŒ€í™” íŒŒì¼ì˜ ì´ë¦„ì„ ë³€ê²½í•˜ê³ , ì¤‘ë³µ ì‹œ ìˆ«ìë¥¼ ë¶™ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    HISTORY_DIR = get_user_history_dir()
    clean_base_name = new_filename_base.strip()
    if not clean_base_name:
        st.error("íŒŒì¼ ì´ë¦„ì€ ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    new_filename = f"{clean_base_name}.json"
    old_path = HISTORY_DIR / old_filename
    new_path = HISTORY_DIR / new_filename

    if old_path == new_path: # ì´ë¦„ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í•¨ìˆ˜ ì¢…ë£Œ
        return

    final_path = new_path
    final_filename = new_filename

    if final_path.exists():
        st.info(f"'{new_filename}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬, ë’¤ì— ìˆ«ìë¥¼ ë¶™ì—¬ ì €ì¥í•©ë‹ˆë‹¤.")
        counter = 1
        while True:
            unique_base_name = f"{clean_base_name} ({counter})"
            unique_filename = f"{unique_base_name}.json"
            unique_path = HISTORY_DIR / unique_filename
            if not unique_path.exists():
                final_path = unique_path
                final_filename = unique_filename
                break
            counter += 1

    try:
        old_path.rename(final_path)
        st.toast(f"'{old_filename}'ì„ '{final_filename}'(ìœ¼)ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
        if st.session_state.get("current_chat_file") == old_filename:
            st.session_state.current_chat_file = final_filename
    except Exception as e:
        st.error(f"íŒŒì¼ ì´ë¦„ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì—¬ ìƒëµ) ---
# --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---
async def plan_mcp_execution(query: str, servers_config: Dict) -> List[List[str]]:
    """ì‚¬ìš©ì ì§ˆì˜ì™€ ë„êµ¬ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê³„íš(ìˆœì°¨/ë³‘ë ¬)ì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
    llm = get_llm()
    active_servers = {name: config for name, config in servers_config.items() if config.get("active", True)}

    if not active_servers:
        st.info("í˜„ì¬ í™œì„±í™”ëœ MCP ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    system_prompt = """You are an expert AI assistant that plans the execution flow for user requests using available tools.
    Analyze the user's query and the descriptions of available tools (MCP servers).
    Determine which tools are needed and the order of execution.

    Rules:
    1. If tasks depend on each other (e.g., Output of A is needed for B), schedule them sequentially.
    2. If tasks are independent (e.g., Compare A and B), schedule them in parallel (in the same step).
    3. Return the plan strictly as a JSON list of lists of server names.
       Example: [["server_A"], ["server_B", "server_C"], ["server_D"]]
       - Step 1: server_A runs.
       - Step 2: server_B and server_C run in parallel (after Step 1 finishes).
       - Step 3: server_D runs (after Step 2 finishes).
    4. If no tools are needed, return an empty list [].
    5. Only use the server names provided in the tool list. Do not invent new names.
    """
    
    prompt_template = """
    [Available Tools]
    {tools_description}

    [User Query]
    {user_query}

    [Execution Plan (JSON)]
    """
    
    descriptions = "\n".join([f"- {name}: {config['description']}" for name, config in active_servers.items()])
    prompt = ChatPromptTemplate.from_template(prompt_template).format(
        tools_description=descriptions,
        user_query=query
    )
    
    try:
        response = await llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=prompt)])
        content = response.content.strip()
        # JSON íŒŒì‹± ì‹œë„ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if content.startswith("```json"):
            content = content[7:]
        if content.endswith("```"):
            content = content[:-3]
        
        plan = json.loads(content.strip())
        
        # ìœ íš¨ì„± ê²€ì‚¬: ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ í™•ì¸
        if isinstance(plan, list):
            validated_plan = []
            for step in plan:
                if isinstance(step, list):
                    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì„œë²„ë§Œ í•„í„°ë§
                    valid_servers = [s for s in step if s in active_servers]
                    if valid_servers:
                        validated_plan.append(valid_servers)
                elif isinstance(step, str) and step in active_servers:
                     # í˜¹ì‹œ ["A", "B"] ì²˜ëŸ¼ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ì¤¬ì„ ê²½ìš° ëŒ€ë¹„ (ëª¨ë‘ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ìˆœì°¨ë¡œ ì²˜ë¦¬? -> ì—¬ê¸°ì„  ë‹¨ì¼ ë‹¨ê³„ë¡œ ê°„ì£¼)
                     validated_plan.append([step])
            return validated_plan
        return []
    except Exception as e:
        st.error(f"ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# (â˜…â˜…â˜…â˜…â˜… ë¡œì§ ìˆ˜ì • â˜…â˜…â˜…â˜…â˜…)
async def process_query(query: str, chat_history: List) -> AsyncGenerator[str, None]:
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ ì„œë²„ ì„ íƒ, ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰ì˜ ì „ì²´ ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    'cancel scope' ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹¤í–‰ ë°©ì‹ì„ ainvokeë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    """

    # <<< [ìˆ˜ì •] ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ë¡œì§ ì‹œì‘ >>>
    MAX_HISTORY_TOKENS = 8192  # LLMì— ì „ë‹¬í•  ìµœëŒ€ íˆìŠ¤í† ë¦¬ í† í° ìˆ˜ ì œí•œ

    history_for_llm = []
    current_tokens = 0

    # ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ìµœì‹ ìˆœìœ¼ë¡œ ìˆœíšŒí•˜ë©° í† í° ìˆ˜ë¥¼ í™•ì¸
    for message in reversed(chat_history):
        message_content = message.content
        # í˜„ì¬ ë©”ì‹œì§€ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°
        message_tokens = count_tokens(message_content)

        # ì´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ë©´ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ëŠ”ì§€ í™•ì¸
        if current_tokens + message_tokens > MAX_HISTORY_TOKENS:
            # ë„˜ëŠ”ë‹¤ë©´ ë” ì´ìƒ ì´ì „ ê¸°ë¡ì„ ì¶”ê°€í•˜ì§€ ì•Šê³  ì¢…ë£Œ
            break

        # í† í° ìˆ˜ ì œí•œì„ ë„˜ì§€ ì•Šìœ¼ë©´ ê¸°ë¡ì— ì¶”ê°€ (ì›ë³¸ ìˆœì„œë¥¼ ìœ„í•´ ë§¨ ì•ì— ì‚½ì…)
        history_for_llm.insert(0, message)
        current_tokens += message_tokens
    # <<< [ìˆ˜ì •] ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ë¡œì§ ë >>>

    mcp_config = load_mcp_config()["mcpServers"]
    llm = get_llm()

    # 1. ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ (ë¼ìš°íŒ…)
    st.write("`1. AIê°€ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½ ì¤‘ì…ë‹ˆë‹¤...`")
    execution_plan = await plan_mcp_execution(query, mcp_config)

    # 2. ì—°ê²°í•  MCP ì„œë²„ê°€ ì—†ì„ ê²½ìš° (ê³„íšì´ ë¹„ì–´ìˆìŒ), LLMìœ¼ë¡œ ì§ì ‘ ì§ˆì˜
    if not execution_plan:
        st.info("âœ… LLMì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤.")
        async for chunk in llm.astream(history_for_llm + [HumanMessage(content=query)]):
            yield chunk.content
        return

    # 3. ê³„íšì— ë”°ë¥¸ ë‹¨ê³„ë³„ ì‹¤í–‰
    st.write(f"`2. ìˆ˜ë¦½ëœ ê³„íš: {execution_plan}`")
    
    accumulated_results = [] # ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì €ì¥
    final_responses = {} # ìµœì¢… ì¢…í•©ì„ ìœ„í•œ ì‘ë‹µ ì €ì¥

    for step_idx, current_step_servers in enumerate(execution_plan):
        step_num = step_idx + 1
        st.write(f"`Step {step_num}: {', '.join(current_step_servers)} ì‹¤í–‰ ì¤‘...`")
        
        # ì´ì „ ë‹¨ê³„ê¹Œì§€ì˜ ê²°ê³¼ ìš”ì•½
        previous_context = ""
        if accumulated_results:
            previous_context = "\n\n[ì´ì „ ë‹¨ê³„ ì²˜ë¦¬ ê²°ê³¼]\n" + "\n".join(accumulated_results)

        async def run_agent_step(name: str, context: str) -> tuple[str, str]:
            """ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)"""
            config = mcp_config[name]
            final_output = f"[{name}] ì‘ë‹µ ì—†ìŒ"
            
            try:
                conn_type = config.get("transport")
                
                async def process_session(read, write):
                    nonlocal final_output
                    async with ClientSession(read, write) as session:
                        await session.initialize()
                        tools = await load_mcp_tools(session)
                        if not tools:
                            return f"[{name}] ë„êµ¬ ì—†ìŒ"
                        
                        agent = create_react_agent(llm, tools)
                        
                        # ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ êµ¬ì„±
                        # ì´ì „ íˆìŠ¤í† ë¦¬ + (ì´ì „ ë‹¨ê³„ ê²°ê³¼ê°€ í¬í•¨ëœ ì‹œìŠ¤í…œ ë©”ì‹œì§€) + í˜„ì¬ ì¿¼ë¦¬
                        system_msg = "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."
                        if context:
                            system_msg += f" ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰ëœ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”:\n{context}"
                        
                        step_messages = history_for_llm + [
                            SystemMessage(content=system_msg),
                            HumanMessage(content=query)
                        ]
                        
                        result = await agent.ainvoke({"messages": step_messages})
                        
                        if 'output' in result:
                            final_output = result['output']
                        elif 'messages' in result and isinstance(result['messages'][-1], AIMessage):
                            final_output = result['messages'][-1].content
                            
                if conn_type == "stdio":
                    params = StdioServerParameters(command=config.get("command"), args=config.get("args", []))
                    async with stdio_client(params) as (read, write):
                        await process_session(read, write)
                elif conn_type == "sse":
                    url = config.get("url")
                    headers = config.get("headers", {})
                    async with sse_client(url, headers=headers) as (read, write):
                        await process_session(read, write)
                        
            except Exception as e:
                final_output = f"[{name}] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
                st.error(f"âŒ '{name}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            return name, final_output

        # í˜„ì¬ ë‹¨ê³„ì˜ ì„œë²„ë“¤ ë³‘ë ¬ ì‹¤í–‰
        tasks = [run_agent_step(name, previous_context) for name in current_step_servers]
        results = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ì²˜ë¦¬
        for name, output in results:
            # ê²°ê³¼ ëˆ„ì  (ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´)
            accumulated_results.append(f"Server '{name}' Output: {output}")
            
            # ìµœì¢… ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥ (ë§ˆì§€ë§‰ ì¢…í•©ì„ ìœ„í•´)
            # í† í° ì œí•œ ì²˜ë¦¬
            MAX_RESPONSE_TOKENS = 1500
            if count_tokens(output) > MAX_RESPONSE_TOKENS:
                 final_responses[name] = output[:3000] + "...(ìƒëµ)" # ëŒ€ëµì ì¸ ê¸¸ì´ë¡œ ìë¦„ (ì •í™•í•œ í† í° ìë¥´ê¸°ëŠ” ìƒëµí•˜ì—¬ ì†ë„ í–¥ìƒ)
            else:
                 final_responses[name] = output
            
            with st.expander(f"Step {step_num} - {name} ê²°ê³¼ í™•ì¸"):
                st.write(output)

    # 4. ìµœì¢… ë‹µë³€ ì¢…í•©
    st.write("`3. ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ. ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...`")
    
    history_str = "\n".join([f"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}" for m in chat_history])
    synthesis_prompt_template = """
    ë‹¹ì‹ ì€ ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ì˜ ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë§ˆìŠ¤í„° AIì…ë‹ˆë‹¤.
    ì•„ë˜ ëŒ€í™” ê¸°ë¡ê³¼ ì‹¤í–‰ ê³„íšì— ë”°ë¥¸ ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {original_query}
    
    [ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼]
    {agent_responses}
    
    [ì¢…í•©ëœ ìµœì¢… ë‹µë³€]
    """
    synthesis_prompt = ChatPromptTemplate.from_template(synthesis_prompt_template)
    synthesis_chain = synthesis_prompt | llm | StrOutputParser()
    
    # agent_responsesë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
    formatted_responses = json.dumps(final_responses, ensure_ascii=False, indent=2)
    if accumulated_results:
         formatted_responses = "\n".join(accumulated_results)

    async for chunk in synthesis_chain.astream({
        "chat_history": history_str,
        "original_query": query,
        "agent_responses": formatted_responses
    }):
        yield chunk


# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="MCP Client on Streamlit", layout="wide")
st.title("ğŸ¤– MCP Client")

# --- 1. ì¸ì¦ ì²˜ë¦¬ (ìˆ˜ì •ëœ ë¡œì§) ---
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

localS = LocalStorage()

if not st.session_state.authenticated:
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì‚¬ìš©ì ì •ë³´ ë¡œë“œ
    credentials_str = os.getenv("USER_CREDENTIALS", "")
    credentials = {}
    if credentials_str:
        for pair in credentials_str.split(','):
            try:
                username, password = pair.strip().split('|', 1)
                credentials[username] = password
            except ValueError:
                st.error("USER_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. 'id|pw,id2|pw2' í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                st.stop()
    
    if not credentials:
        st.error("ë¡œê·¸ì¸ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. USER_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    st.subheader("ë¡œê·¸ì¸")
    
    # localStorageì—ì„œ ì €ì¥ëœ ì‚¬ìš©ì ID ë¶ˆëŸ¬ì˜¤ê¸°
    remembered_username = localS.getItem("remembered_username") or ""
    
    # ì €ì¥ëœ ì•„ì´ë””ê°€ ìˆìœ¼ë©´ ì²´í¬ë°•ìŠ¤ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì„ íƒ ìƒíƒœë¡œ ë‘¡ë‹ˆë‹¤.
    is_checked_by_default = remembered_username != ""
    
    username = st.text_input("ì‚¬ìš©ì ì•„ì´ë””", value=remembered_username)
    remember_id = st.checkbox("ì•„ì´ë”” ì €ì¥", value=is_checked_by_default)
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
    
    if st.button("ë¡œê·¸ì¸"):
        if username in credentials and credentials[username] == password:
            st.session_state.authenticated = True
            st.session_state.username = username
            
            # 'ì•„ì´ë”” ì €ì¥' ì²´í¬ë°•ìŠ¤ ìƒíƒœì— ë”°ë¼ localStorageì— ì €ì¥ ë˜ëŠ” ì‚­ì œ
            if remember_id:
                localS.setItem("remembered_username", username)
            else:
                localS.setItem("remembered_username", "") # ì €ì¥ëœ ì•„ì´ë”” ì‚­ì œ

            # (â˜…â˜…â˜… ìˆ˜ì •ëœ ë¶€ë¶„ â˜…â˜…â˜…)
            # localStorageê°€ ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ ì•„ì£¼ ì§§ì€ ì§€ì—° ì‹œê°„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            time.sleep(0.1)
            
            st.rerun()
        else:
            st.error("ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# --- 2. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (ì¸ì¦ í›„) ---
with st.sidebar:
    st.header(f"í™˜ì˜í•©ë‹ˆë‹¤, {st.session_state.username}ë‹˜!")
    if st.button("ë¡œê·¸ì•„ì›ƒ"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

    def start_new_chat():
        st.session_state.messages = []
        st.session_state.current_chat_file = None

    def auto_save_chat():
        HISTORY_DIR = get_user_history_dir()
        if st.session_state.get("current_chat_file") and st.session_state.get("messages"):
            save_path = HISTORY_DIR / st.session_state.current_chat_file
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)

    def load_chat(filename: str):
        HISTORY_DIR = get_user_history_dir()
        load_path = HISTORY_DIR / filename
        with open(load_path, "r", encoding="utf-8") as f:
            st.session_state.messages = json.load(f)
        st.session_state.current_chat_file = filename

    def delete_chat(filename: str):
        HISTORY_DIR = get_user_history_dir()
        if st.session_state.get("current_chat_file") == filename:
            start_new_chat()
        file_to_delete = HISTORY_DIR / filename
        if file_to_delete.exists():
            file_to_delete.unlink()
            st.toast(f"'{filename}'ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    st.button("ìƒˆë¡œìš´ ì±„íŒ… ì—´ê¸°", on_click=start_new_chat, use_container_width=True)
    st.divider()

    # LLM ê´€ë¦¬ UI (ê¸°ì¡´ê³¼ ë™ì¼)
    saved_model = localS.getItem("selected_model")
    saved_category = saved_model[0] if saved_model else ""
    saved_item = saved_model[1] if saved_model else ""
    
    categories = list(llm_options.keys())
    category_index = categories.index(saved_category) if saved_category in categories else 0
    
    st.header("LLM ê´€ë¦¬")
    selected_category = st.selectbox("LLMë¥¼ ì„ íƒí•˜ì„¸ìš”:", categories, index=category_index)
    
    model_options = llm_options[selected_category]
    item_index = model_options.index(saved_item) if saved_item in model_options else 0
    selected_item = st.selectbox(f"{selected_category} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”:", model_options, index=item_index)
    localS.setItem("selected_model", [selected_category,selected_item])

    st.divider()
    st.header(f"MCP ì„œë²„ ê´€ë¦¬ ({st.session_state.username})")
    mcp_config = load_mcp_config()
    with st.expander("ì„œë²„ ëª©ë¡ ë³´ê¸°/ê´€ë¦¬"):
        st.json(mcp_config, expanded=False)
        servers = list(mcp_config["mcpServers"].keys())
        server_to_delete = st.selectbox("ì‚­ì œí•  ì„œë²„ ì„ íƒ", [""] + servers)
        if st.button("ì„ íƒëœ ì„œë²„ ì‚­ì œ", type="primary"):
            if server_to_delete and server_to_delete in mcp_config["mcpServers"]:
                del mcp_config["mcpServers"][server_to_delete]
                save_mcp_config(mcp_config)
                st.success(f"'{server_to_delete}' ì„œë²„ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                time.sleep(1); st.rerun()
        st.markdown("---")
        st.write("**ì„œë²„ ìŠ¤ìœ„ì¹˜**")
        server_configs = mcp_config.get("mcpServers", {})
        config_changed = False
        for server_name, config in server_configs.items():
            is_active = st.toggle(
                server_name,
                value=config.get("active", True),
                key=f"toggle_{server_name}"
            )
            if is_active != config.get("active", True):
                mcp_config["mcpServers"][server_name]["active"] = is_active
                config_changed = True
        if config_changed:
            save_mcp_config(mcp_config)
            st.toast("ì„œë²„ í™œì„±í™” ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.markdown("---")
        st.write("**ìƒˆ ì„œë²„ ì¶”ê°€**")
        new_server_name = st.text_input("ìƒˆ ì„œë²„ ì´ë¦„")
        new_server_config_str = st.text_area("ìƒˆ ì„œë²„ JSON ì„¤ì •", height=200, placeholder='{\n  "description": "...",\n ...}')
        if st.button("ìƒˆ ì„œë²„ ì¶”ê°€"):
            if new_server_name and new_server_config_str:
                try:
                    new_config = json.loads(new_server_config_str)
                    mcp_config["mcpServers"][new_server_name] = new_config
                    save_mcp_config(mcp_config)
                    st.success(f"'{new_server_name}' ì„œë²„ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    time.sleep(1); st.rerun()
                except json.JSONDecodeError: st.error("ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
            else: st.warning("ì„œë²„ ì´ë¦„ê³¼ ì„¤ì •ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()
    st.header("ì €ì¥ëœ ëŒ€í™”")

    # ëŒ€í™” ëª©ë¡ ê´€ë¦¬ UI (ê¸°ì¡´ê³¼ ë™ì¼, ê²½ë¡œë§Œ ìˆ˜ì •ë¨)
    HISTORY_DIR = get_user_history_dir()
    if "editing_chat_file" not in st.session_state:
        st.session_state.editing_chat_file = None
    # ... (display_chat_item, show_all_chats_dialog ë“± ëŒ€í™” ëª©ë¡ UI í•¨ìˆ˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼) ...
    def display_chat_item(filename: str, key_prefix: str):
        """ëŒ€í™” ëª©ë¡ ì•„ì´í…œì„ í‘œì‹œí•˜ê³  ìˆ˜ì •/ì‚­ì œ UIë¥¼ ì œê³µí•˜ëŠ” í•¨ìˆ˜"""
        is_editing = st.session_state.get("editing_chat_file") == filename

        if is_editing:
            # ì´ë¦„ ìˆ˜ì • ëª¨ë“œ UI
            c1, c2, c3 = st.columns([0.7, 0.15, 0.15])
            with c1:
                new_name_base = st.text_input(
                    "ìƒˆ íŒŒì¼ ì´ë¦„",
                    value=filename.removesuffix(".json"),
                    key=f"text_{key_prefix}_{filename}",
                    label_visibility="collapsed"
                )
            with c2:
                if st.button("ì €ì¥", key=f"save_{key_prefix}_{filename}", use_container_width=True, type="primary"):
                    rename_chat(filename, st.session_state[f"text_{key_prefix}_{filename}"])
                    st.session_state.editing_chat_file = None
                    st.rerun()
            with c3:
                if st.button("ì·¨ì†Œ", key=f"cancel_{key_prefix}_{filename}", use_container_width=True):
                    st.session_state.editing_chat_file = None
                    st.rerun()
        else:
            # ì¼ë°˜ í‘œì‹œ ëª¨ë“œ UI
            c1, c2, c3 = st.columns([0.75, 0.125, 0.125])
            with c1:
                is_active_chat = st.session_state.get("current_chat_file") == filename
                button_type = "primary" if is_active_chat else "secondary"
                if st.button(filename, key=f"load_{key_prefix}_{filename}", use_container_width=True, type=button_type):
                    if not is_active_chat:
                        load_chat(filename)
                        st.session_state.editing_chat_file = None
                        st.rerun()
            with c2:
                if st.button("âœï¸", key=f"edit_{key_prefix}_{filename}", use_container_width=True, help="ì´ë¦„ ë³€ê²½"):
                    st.session_state.editing_chat_file = filename
                    st.rerun()
            with c3:
                if st.button("X", key=f"delete_{key_prefix}_{filename}", use_container_width=True, help=f"{filename} ì‚­ì œ"):
                    delete_chat(filename)
                    st.rerun()

    try:
        saved_chats_paths = [p for p in HISTORY_DIR.glob("*.json")]
        saved_chats_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        saved_chats = [p.name for p in saved_chats_paths]
    except FileNotFoundError:
        saved_chats = []

    @st.dialog("ì „ì²´ ëŒ€í™” ëª©ë¡")
    def show_all_chats_dialog(older_chats_list):
        st.write(f"ì´ {len(saved_chats)}ê°œì˜ ëŒ€í™”ê°€ ìˆìŠµë‹ˆë‹¤.")
        items_to_show_count = st.session_state.get("dialog_items_to_show", 10)
        chats_to_display = older_chats_list[:items_to_show_count]
        for filename in chats_to_display:
            display_chat_item(filename, key_prefix="dialog")
        st.divider()
        if len(older_chats_list) > items_to_show_count:
            if st.button("ë”ë³´ê¸°", use_container_width=True):
                st.session_state.dialog_items_to_show += 10
                st.rerun()
        if st.button("ë‹«ê¸°", use_container_width=True, type="primary"):
            st.session_state.show_all_chats = False
            st.session_state.editing_chat_file = None
            st.rerun()
    
    if not saved_chats:
        st.write("ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        recent_chats = saved_chats[:10]
        older_chats = saved_chats[10:]
        for filename in recent_chats:
            display_chat_item(filename, key_prefix="recent")
        if older_chats:
            if st.button("ë” ë³´ê¸°...", use_container_width=True):
                st.session_state.show_all_chats = True
                st.session_state.dialog_items_to_show = 10
                st.rerun()
    
    if st.session_state.get("show_all_chats"):
        older_chats = saved_chats[10:]
        show_all_chats_dialog(older_chats)


# --- ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_chat_file" not in st.session_state:
    st.session_state.current_chat_file = None

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

st.markdown(
    """
    <style>
    @media(max-width:1024px){
        .stBottom{
        bottom:60px;
        }
    }
    </style>
    """,unsafe_allow_html=True
)
prompt = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
if prompt:
    if not st.session_state.get("current_chat_file"):
        st.session_state.current_chat_file = generate_filename_with_timestamp()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        history = [
            HumanMessage(content=m['content']) if m['role'] == 'user' else AIMessage(content=m['content'])
            for m in st.session_state.messages[:-1]
        ]
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
        # í•µì‹¬ ë¡œì§ í•¨ìˆ˜(process_query)ê°€ ìƒëµë˜ì—ˆìœ¼ë¯€ë¡œ,
        # ì›ë³¸ ì½”ë“œì˜ process_query í•¨ìˆ˜ ì „ì²´ë¥¼ ìœ„ì— ë¶™ì—¬ë„£ì–´ì•¼ ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤.
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
        response = st.write_stream(process_query(prompt, history))
        st.badge("Answer by "+selected_item+"", icon=":material/check:", color="green")

    st.session_state.messages.append({"role": "assistant", "content": response})
    auto_save_chat()