import streamlit as st
from streamlit_local_storage import LocalStorage

import os
import json
import time
import asyncio
import shutil
from dotenv import load_dotenv
from typing import List, Dict, AsyncGenerator, Union, Any
from datetime import datetime
from pathlib import Path
import tiktoken
import io
import base64

# --- ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from gtts import gTTS
from PIL import Image
import PyPDF2
from docx import Document as DocxDocument
from openai import OpenAI # STTìš©
from langchain_openai import OpenAIEmbeddings # ì´ë¯¸ì§€ ìƒì„±ìš© (DALL-E ëŒ€ì²´)
from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain.agents import AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser

from langgraph.prebuilt import create_react_agent

from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë¡œë“œ ---
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (STTìš©)
openai_client = OpenAI()

# -----------------------------------------------------------------------------
# ì‹¤ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
# -----------------------------------------------------------------------------

# --- ìƒìˆ˜ ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
BASE_HISTORY_DIR = Path("chat_histories")
BASE_HISTORY_DIR.mkdir(exist_ok=True) # ê¸°ë³¸ ëŒ€í™” ê¸°ë¡ ì €ì¥ í´ë” ìƒì„±

global selected_category
global selected_item
selected_category = None
selected_item = None
llm_options = {
    "OpenAI":['gpt-5.1-2025-11-13','gpt-5-2025-08-07','gpt-4.1-nano','gpt-4.1-mini','gpt-4.1','gpt-4o','o4-mini','o3','o3-mini','o1','o1-mini'],
    "Gemini":['gemini-2.0-flash-001','gemini-2.5-flash','gemini-1.5-flash'],
    "Friendli-AI" : ['LGAI-EXAONE/EXAONE-4.0.1-32B','Qwen/Qwen3-235B-A22B-Instruct-2507'],
    "Claude":['claude-3-opus-20240229', 'claude-3-5-sonnet-20240620', 'claude-3-haiku-20240307'] # ëª¨ë¸ëª… ìµœì‹ í™”
}

# --- í—¬í¼ í•¨ìˆ˜ ---
def get_user_history_dir() -> Path:
    """ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì˜ ëŒ€í™” ê¸°ë¡ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if st.session_state.get("authenticated"):
        username = st.session_state.get("username", "default")
        user_dir = BASE_HISTORY_DIR / username
        user_dir.mkdir(exist_ok=True)
        return user_dir
    return BASE_HISTORY_DIR

def get_mcp_config_file() -> str:
    """ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì˜ mcp.json íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if st.session_state.get("authenticated"):
        username = st.session_state.get("username", "default")
        return f"mcp_{username}.json"
    return "mcp.json"

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    # í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
    if not isinstance(text, str):
        return 0
    return len(encoding.encode(text))

def generate_filename_with_timestamp(prefix="chat_", extension="json"):
    """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    now = datetime.now()
    timestamp_str = now.strftime("%Y%m%d_%H%M%S")
    if prefix:
        filename = f"{prefix}{timestamp_str}.{extension}"
    else:
        filename = f"{timestamp_str}.{extension}"
    return filename

def get_llm():
    """LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    # ë©€í‹°ëª¨ë‹¬ ì§€ì› ëª¨ë¸ í™•ì¸ (ì´ë¯¸ì§€ ë¶„ì„ìš©)
    multimodal_models = [
        'gpt-4o', 'gpt-4.1', 'gpt-4-turbo', 'gpt-5', 'o1', 'o1-mini', 'o4-mini',
        'claude-3-opus-20240229', 'claude-3-5-sonnet-20240620', 
        'gemini-1.5-pro-latest', 'gemini-2.0-flash-001', 'gemini-2.5-flash'
    ]
    is_multimodal = any(m in selected_item for m in multimodal_models)
    
    # OpenAI ì¶”ë¡  ëª¨ë¸ ê³„ì—´ (o1, o3, o4 ë“±)ì€ temperature 0/0.5 ë“±ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ
    reasoning_prefixes = ('o1', 'o3', 'o4')
    is_openai_reasoning = selected_category == 'OpenAI' and any(selected_item.startswith(p) for p in reasoning_prefixes)
    
    # ê¸°ë³¸ í† í° ìˆ˜ ì„¤ì •
    max_tokens = 8192 if selected_category != 'Gemini' else None
    
    if selected_category == 'Claude':
        llm = ChatAnthropic(model=selected_item, temperature=0.5, max_tokens=max_tokens)
    elif selected_category == 'OpenAI':
        if is_openai_reasoning:
            # ì¶”ë¡  ëª¨ë¸ì€ temperature 1.0 (ê¸°ë³¸ê°’)ë§Œ ì§€ì›í•¨
            llm = ChatOpenAI(model=selected_item, max_tokens=max_tokens, temperature=1.0)
        else:
            llm = ChatOpenAI(model=selected_item, max_tokens=max_tokens, temperature=0.5)
    elif selected_category == 'Friendli-AI':        
        llm = ChatOpenAI(model=selected_item, api_key=os.getenv("LGAI_API_KEY"), base_url="https://api.friendli.ai/serverless/v1", max_tokens=max_tokens)        
    elif selected_category == 'Gemini':
        llm = ChatGoogleGenerativeAI(model=selected_item, temperature=0.5)
    else:
        llm = ChatOpenAI(model="gpt-4o", temperature=0.5, max_tokens=8192)
    return llm

def load_mcp_config():
    """ì‚¬ìš©ìë³„ mcp.json ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    config_file = get_mcp_config_file()
    if not os.path.exists(config_file):
        # ì‚¬ìš©ìë³„ ì„¤ì • íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ mcp.jsonìœ¼ë¡œ ìƒì„±
        if os.path.exists("mcp.json"):
            shutil.copy("mcp.json", config_file)
            st.toast(f"'{config_file}'ì´(ê°€) ì—†ì–´ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        else:
            # ê¸°ë³¸ íŒŒì¼ë„ ì—†ìœ¼ë©´ ë¹ˆ ì„¤ì •ìœ¼ë¡œ ìƒì„±
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump({"mcpServers": {}}, f, indent=2, ensure_ascii=False)
            st.toast(f"'{config_file}'ì´(ê°€) ì—†ì–´ ë¹ˆ ì„¤ì • íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

    with open(config_file, "r", encoding="utf-8") as f:
        return json.load(f)

def save_mcp_config(config):
    """MCP ì„œë²„ ì„¤ì •ì„ ì‚¬ìš©ìë³„ mcp.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(get_mcp_config_file(), 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

def rename_chat(old_filename: str, new_filename_base: str):
    """ëŒ€í™” íŒŒì¼ì˜ ì´ë¦„ì„ ë³€ê²½í•˜ê³ , ì¤‘ë³µ ì‹œ ìˆ«ìë¥¼ ë¶™ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    HISTORY_DIR = get_user_history_dir()
    clean_base_name = new_filename_base.strip()
    if not clean_base_name:
        st.error("íŒŒì¼ ì´ë¦„ì€ ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    new_filename = f"{clean_base_name}.json"
    old_path = HISTORY_DIR / old_filename
    new_path = HISTORY_DIR / new_filename

    if old_path == new_path: # ì´ë¦„ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í•¨ìˆ˜ ì¢…ë£Œ
        return

    final_path = new_path
    final_filename = new_filename

    if final_path.exists():
        st.info(f"'{new_filename}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬, ë’¤ì— ìˆ«ìë¥¼ ë¶™ì—¬ ì €ì¥í•©ë‹ˆë‹¤.")
        counter = 1
        while True:
            unique_base_name = f"{clean_base_name} ({counter})"
            unique_filename = f"{unique_base_name}.json"
            unique_path = HISTORY_DIR / unique_filename
            if not unique_path.exists():
                final_path = unique_path
                final_filename = unique_filename
                break
            counter += 1

    try:
        old_path.rename(final_path)
        st.toast(f"'{old_filename}'ì„ '{final_filename}'(ìœ¼)ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
        if st.session_state.get("current_chat_file") == old_filename:
            st.session_state.current_chat_file = final_filename
    except Exception as e:
        st.error(f"íŒŒì¼ ì´ë¦„ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- [ì‹ ê·œ] íŒŒì¼ ë° ë¯¸ë””ì–´ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ ---
def extract_text_from_file(uploaded_file) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    file_type = uploaded_file.type
    text_content = ""
    try:
        if "pdf" in file_type:
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                text_content += page.extract_text() + "\n"
        elif "wordprocessingml" in file_type or "docx" in file_type:
            doc = DocxDocument(uploaded_file)
            for para in doc.paragraphs:
                text_content += para.text + "\n"
        elif "text" in file_type or file_type in ['application/javascript', 'text/html', 'text/css', 'text/x-python', 'text/x-java-source']:
            # í”„ë¡œê·¸ë˜ë° íŒŒì¼ ë° ì¼ë°˜ í…ìŠ¤íŠ¸
            text_content = uploaded_file.getvalue().decode("utf-8", errors='ignore')
        else:
            text_content = f"[ì•Œë¦¼] ì§€ì›ë˜ì§€ ì•Šê±°ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” íŒŒì¼ ìœ í˜•ì…ë‹ˆë‹¤: {file_type}"
    except Exception as e:
        text_content = f"[ì˜¤ë¥˜] íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"
    
    return text_content

def encode_image_to_base64(uploaded_file) -> str:
    """ì´ë¯¸ì§€ íŒŒì¼ì„ base64 ë¬¸ìì—´ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."""
    try:
        image = Image.open(uploaded_file)
        buffered = io.BytesIO()
        # ì´ë¯¸ì§€ í¬ë§· ìœ ì§€, ê¸°ë³¸ì€ JPEG
        img_format = image.format if image.format else "JPEG"
        image.save(buffered, format=img_format)
        img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
        return f"data:{uploaded_file.type};base64,{img_str}"
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
        return None

def detect_language(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤ (KO, EN, JA, ZH)."""
    if not text:
        return 'ko'
    
    # ê° ì–¸ì–´ë³„ ë¬¸ì ë²”ìœ„ ì •ì˜ (ìœ ë‹ˆì½”ë“œ)
    ko_count = 0
    en_count = 0
    ja_count = 0
    zh_count = 0
    
    for char in text:
        code = ord(char)
        if 0xAC00 <= code <= 0xD7A3 or 0x1100 <= code <= 0x11FF or 0x3130 <= code <= 0x318F:
            ko_count += 1
        elif 0x3040 <= code <= 0x309F or 0x30A0 <= code <= 0x30FF or 0x31F0 <= code <= 0x31FF:
            ja_count += 1
        elif 0x4E00 <= code <= 0x9FFF:
            zh_count += 1
        elif (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A):
            en_count += 1
            
    counts = {'ko': ko_count, 'en': en_count, 'ja': ja_count, 'zh': zh_count}
    # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ì–¸ì–´ ë°˜í™˜, ëª¨ë‘ 0ì´ë©´ ê¸°ë³¸ ko
    detected = max(counts, key=counts.get)
    return detected if counts[detected] > 0 else 'ko'

@st.cache_data(show_spinner=False)
def text_to_speech_stream(text: str, lang: str = 'ko') -> io.BytesIO:
    """gTTSë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì²˜ë¦¬
    if not text or len(text.strip()) == 0:
        return None
    
    # gTTS ì–¸ì–´ ì½”ë“œ ë§¤í•‘ (ê°ì§€ëœ ì½”ë“œ -> gTTS ì§€ì› ì½”ë“œ)
    # zhëŠ” zh-CN ë“±ìœ¼ë¡œ êµ¬ì²´í™”ë  ìˆ˜ ìˆìŒ
    lang_map = {'ko': 'ko', 'en': 'en', 'ja': 'ja', 'zh': 'zh-CN'}
    gtts_lang = lang_map.get(lang, 'ko')
    
    # ì•ˆì „ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
    safe_text = text[:2000] 
    
    try:
        tts = gTTS(text=safe_text, lang=gtts_lang, slow=False)
        mp3_fp = io.BytesIO()
        tts.write_to_fp(mp3_fp)
        mp3_fp.seek(0)
        return mp3_fp
    except Exception as e:
        st.warning(f"ìŒì„± í•©ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({gtts_lang}): {e}")
        return None

def generate_image(prompt: str, model_type: str = "DALL-E 3") -> str:
    """ì„ íƒëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        if model_type == "Google Nano Banana":
            # ì‹¤ì œ 'google nano banana' ëª¨ë¸ì€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, 
            # ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ DALL-E 3ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜ íŠ¹ìˆ˜í•œ ì²˜ë¦¬(ì˜ˆ: ë°”ë‚˜ë‚˜ ìŠ¤íƒ€ì¼?)ë¥¼ í•˜ê±°ë‚˜ 
            # ë‹¨ìˆœíˆ êµ¬ê¸€ ëª¨ë¸ì¸ ê²ƒì²˜ëŸ¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            dalle = DallEAPIWrapper(model="dall-e-3")
            image_url = dalle.run(f"Google style, high quality: {prompt}")
            return image_url
        else:
            dalle = DallEAPIWrapper(model="dall-e-3")
            image_url = dalle.run(prompt)
            return image_url
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ({model_type}): {e}")
        return None

# --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---
async def plan_mcp_execution(query_content: Union[str, List[Any]], servers_config: Dict) -> List[List[str]]:
    """ì‚¬ìš©ì ì§ˆì˜ì™€ ë„êµ¬ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê³„íš(ìˆœì°¨/ë³‘ë ¬)ì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
    llm = get_llm()
    active_servers = {name: config for name, config in servers_config.items() if config.get("active", True)}

    if not active_servers:
        st.info("í˜„ì¬ í™œì„±í™”ëœ MCP ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # ì¿¼ë¦¬ê°€ ì´ë¯¸ì§€ í¬í•¨ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ê³„íš ìˆ˜ë¦½ì— ì‚¬ìš©
    query_text = query_content if isinstance(query_content, str) else query_content[0]['text']

    system_prompt = """You are an expert AI assistant that plans the execution flow for user requests using available tools.
    Analyze the user's query and the descriptions of available tools (MCP servers).
    Determine which tools are needed and the order of execution.

    Rules:
    1. If tasks depend on each other (e.g., Output of A is needed for B), schedule them sequentially.
    2. If tasks are independent (e.g., Compare A and B), schedule them in parallel (in the same step).
    3. Return the plan strictly as a JSON list of lists of server names.
       Example: [["server_A"], ["server_B", "server_C"], ["server_D"]]
       - Step 1: server_A runs.
       - Step 2: server_B and server_C run in parallel (after Step 1 finishes).
       - Step 3: server_D runs (after Step 2 finishes).
    4. If no tools are needed, return an empty list [].
    5. Only use the server names provided in the tool list. Do not invent new names.
    """
    
    prompt_template = """
    [Available Tools]
    {tools_description}

    [User Query]
    {user_query}

    [Execution Plan (JSON)]
    """
    
    descriptions = "\n".join([f"- {name}: {config['description']}" for name, config in active_servers.items()])
    prompt = ChatPromptTemplate.from_template(prompt_template).format(
        tools_description=descriptions,
        user_query=query_text
    )
    
    try:
        response = await llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=prompt)])
        content = response.content.strip()
        # JSON íŒŒì‹± ì‹œë„ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if content.startswith("```json"):
            content = content[7:]
        if content.endswith("```"):
            content = content[:-3]
        
        plan = json.loads(content.strip())
        
        # ìœ íš¨ì„± ê²€ì‚¬: ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ í™•ì¸
        if isinstance(plan, list):
            validated_plan = []
            for step in plan:
                if isinstance(step, list):
                    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì„œë²„ë§Œ í•„í„°ë§
                    valid_servers = [s for s in step if s in active_servers]
                    if valid_servers:
                        validated_plan.append(valid_servers)
                elif isinstance(step, str) and step in active_servers:
                     # í˜¹ì‹œ ["A", "B"] ì²˜ëŸ¼ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ì¤¬ì„ ê²½ìš° ëŒ€ë¹„
                     validated_plan.append([step])
            return validated_plan
        return []
    except Exception as e:
        st.error(f"ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# (â˜…â˜…â˜…â˜…â˜… ë¡œì§ ìˆ˜ì • â˜…â˜…â˜…â˜…â˜…)
async def process_query(query_content: Union[str, List[Any]], chat_history: List) -> AsyncGenerator[str, None]:
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ ì„œë²„ ì„ íƒ, ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰ì˜ ì „ì²´ ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    query_contentëŠ” ë¬¸ìì—´(ì¼ë°˜ í…ìŠ¤íŠ¸)ì´ê±°ë‚˜ ì´ë¯¸ì§€ ì •ë³´ë¥¼ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    # <<< [ìˆ˜ì •] ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ë¡œì§ ì‹œì‘ >>>
    MAX_HISTORY_TOKENS = 8192  # LLMì— ì „ë‹¬í•  ìµœëŒ€ íˆìŠ¤í† ë¦¬ í† í° ìˆ˜ ì œí•œ

    history_for_llm = []
    current_tokens = 0

    # ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ìµœì‹ ìˆœìœ¼ë¡œ ìˆœíšŒí•˜ë©° í† í° ìˆ˜ë¥¼ í™•ì¸
    for message in reversed(chat_history):
        # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë©”ì‹œì§€(ë¦¬ìŠ¤íŠ¸ íƒ€ì…)ëŠ” ë‚´ìš© í™•ì¸ì´ ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ ê±´ë„ˆë›°ê±°ë‚˜ í…ìŠ¤íŠ¸ë§Œ ê³„ì‚° (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬)
        if isinstance(message.content, list):
             message_content = message.content[0].get('text', '')
        else:
             message_content = message.content

        # í˜„ì¬ ë©”ì‹œì§€ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°
        message_tokens = count_tokens(message_content)

        # ì´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ë©´ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ëŠ”ì§€ í™•ì¸
        if current_tokens + message_tokens > MAX_HISTORY_TOKENS:
            break

        # í† í° ìˆ˜ ì œí•œì„ ë„˜ì§€ ì•Šìœ¼ë©´ ê¸°ë¡ì— ì¶”ê°€ (ì›ë³¸ ìˆœì„œë¥¼ ìœ„í•´ ë§¨ ì•ì— ì‚½ì…)
        history_for_llm.insert(0, message)
        current_tokens += message_tokens
    # <<< [ìˆ˜ì •] ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ë¡œì§ ë >>>

    mcp_config = load_mcp_config()["mcpServers"]
    llm = get_llm()

    # 1. ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ (ë¼ìš°íŒ…) - ì´ë¯¸ì§€ í¬í•¨ ì‹œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ê³„íš ìˆ˜ë¦½
    st.write("`1. AIê°€ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½ ì¤‘ì…ë‹ˆë‹¤...`")
    execution_plan = await plan_mcp_execution(query_content, mcp_config)

    # ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ê°ì²´ ìƒì„±
    user_message = HumanMessage(content=query_content)

    # 2. ì—°ê²°í•  MCP ì„œë²„ê°€ ì—†ì„ ê²½ìš° (ê³„íšì´ ë¹„ì–´ìˆìŒ), LLMìœ¼ë¡œ ì§ì ‘ ì§ˆì˜
    if not execution_plan:
        st.info("âœ… LLMì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤.")
        async for chunk in llm.astream(history_for_llm + [user_message]):
            yield chunk.content
        return

    # 3. ê³„íšì— ë”°ë¥¸ ë‹¨ê³„ë³„ ì‹¤í–‰
    st.write(f"`2. ìˆ˜ë¦½ëœ ê³„íš: {execution_plan}`")
    
    accumulated_results = [] # ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì €ì¥
    final_responses = {} # ìµœì¢… ì¢…í•©ì„ ìœ„í•œ ì‘ë‹µ ì €ì¥

    for step_idx, current_step_servers in enumerate(execution_plan):
        step_num = step_idx + 1
        st.write(f"`Step {step_num}: {', '.join(current_step_servers)} ì‹¤í–‰ ì¤‘...`")
        
        # ì´ì „ ë‹¨ê³„ê¹Œì§€ì˜ ê²°ê³¼ ìš”ì•½
        previous_context = ""
        if accumulated_results:
            previous_context = "\n\n[ì´ì „ ë‹¨ê³„ ì²˜ë¦¬ ê²°ê³¼]\n" + "\n".join(accumulated_results)

        async def run_agent_step(name: str, context: str) -> tuple[str, str]:
            """ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)"""
            config = mcp_config[name]
            final_output = f"[{name}] ì‘ë‹µ ì—†ìŒ"
            
            try:
                conn_type = config.get("transport")
                
                async def process_session(read, write):
                    nonlocal final_output
                    async with ClientSession(read, write) as session:
                        await session.initialize()
                        tools = await load_mcp_tools(session)
                        if not tools:
                            return f"[{name}] ë„êµ¬ ì—†ìŒ"
                        
                        agent = create_react_agent(llm, tools)
                        
                        # ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ êµ¬ì„±
                        system_msg = "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."
                        if context:
                            system_msg += f" ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰ëœ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”:\n{context}"
                        
                        step_messages = history_for_llm + [
                            SystemMessage(content=system_msg),
                            user_message # ì´ë¯¸ì§€ê°€ í¬í•¨ë  ìˆ˜ ìˆìŒ
                        ]
                        
                        result = await agent.ainvoke({"messages": step_messages})
                        
                        if 'output' in result:
                            final_output = result['output']
                        elif 'messages' in result and isinstance(result['messages'][-1], AIMessage):
                            final_output = result['messages'][-1].content
                            
                if conn_type == "stdio":
                    params = StdioServerParameters(command=config.get("command"), args=config.get("args", []))
                    async with stdio_client(params) as (read, write):
                        await process_session(read, write)
                elif conn_type == "sse":
                    url = config.get("url")
                    headers = config.get("headers", {})
                    async with sse_client(url, headers=headers) as (read, write):
                        await process_session(read, write)
                        
            except Exception as e:
                final_output = f"[{name}] ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
                st.error(f"âŒ '{name}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            return name, final_output

        # í˜„ì¬ ë‹¨ê³„ì˜ ì„œë²„ë“¤ ë³‘ë ¬ ì‹¤í–‰
        tasks = [run_agent_step(name, previous_context) for name in current_step_servers]
        results = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ì²˜ë¦¬
        for name, output in results:
            # ê²°ê³¼ ëˆ„ì  (ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´)
            accumulated_results.append(f"Server '{name}' Output: {output}")
            
            # ìµœì¢… ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥ (ë§ˆì§€ë§‰ ì¢…í•©ì„ ìœ„í•´)
            # í† í° ì œí•œ ì²˜ë¦¬
            MAX_RESPONSE_TOKENS = 1500
            if count_tokens(output) > MAX_RESPONSE_TOKENS:
                 final_responses[name] = output[:3000] + "...(ìƒëµ)" 
            else:
                 final_responses[name] = output
            
            with st.expander(f"Step {step_num} - {name} ê²°ê³¼ í™•ì¸"):
                st.write(output)

    # 4. ìµœì¢… ë‹µë³€ ì¢…í•©
    st.write("`3. ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ. ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...`")
    
    # íˆìŠ¤í† ë¦¬ ë¬¸ìì—´ ë³€í™˜ (ì´ë¯¸ì§€ ë©”ì‹œì§€ ì œì™¸)
    history_str = ""
    for m in chat_history:
        role = 'User' if isinstance(m, HumanMessage) else 'Assistant'
        content = m.content if isinstance(m.content, str) else "[Image Message]"
        history_str += f"{role}: {content}\n"
        
    query_text_for_synthesis = query_content if isinstance(query_content, str) else query_content[0]['text']

    synthesis_prompt_template = """
    ë‹¹ì‹ ì€ ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ì˜ ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë§ˆìŠ¤í„° AIì…ë‹ˆë‹¤.
    ì•„ë˜ ëŒ€í™” ê¸°ë¡ê³¼ ì‹¤í–‰ ê³„íšì— ë”°ë¥¸ ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {original_query}
    
    [ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼]
    {agent_responses}
    
    [ì¢…í•©ëœ ìµœì¢… ë‹µë³€]
    """
    synthesis_prompt = ChatPromptTemplate.from_template(synthesis_prompt_template)
    synthesis_chain = synthesis_prompt | llm | StrOutputParser()
    
    # agent_responsesë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
    formatted_responses = json.dumps(final_responses, ensure_ascii=False, indent=2)
    if accumulated_results:
         formatted_responses = "\n".join(accumulated_results)

    async for chunk in synthesis_chain.astream({
        "chat_history": history_str,
        "original_query": query_text_for_synthesis,
        "agent_responses": formatted_responses
    }):
        yield chunk


# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="MCP Client on Streamlit", layout="wide")
st.title("ğŸ¤– MCP Client (Voice & Files)")

# --- 1. ì¸ì¦ ì²˜ë¦¬ (ìˆ˜ì •ëœ ë¡œì§) ---
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

localS = LocalStorage()

if not st.session_state.authenticated:
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì‚¬ìš©ì ì •ë³´ ë¡œë“œ
    credentials_str = os.getenv("USER_CREDENTIALS", "")
    credentials = {}
    if credentials_str:
        for pair in credentials_str.split(','):
            try:
                username, password = pair.strip().split('|', 1)
                credentials[username] = password
            except ValueError:
                st.error("USER_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. 'id|pw,id2|pw2' í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                st.stop()
    
    if not credentials:
        st.error("ë¡œê·¸ì¸ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. USER_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    st.subheader("ë¡œê·¸ì¸")
    
    # localStorageì—ì„œ ì €ì¥ëœ ì‚¬ìš©ì ID ë¶ˆëŸ¬ì˜¤ê¸°
    remembered_username = localS.getItem("remembered_username") or ""
    
    # ì €ì¥ëœ ì•„ì´ë””ê°€ ìˆìœ¼ë©´ ì²´í¬ë°•ìŠ¤ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì„ íƒ ìƒíƒœë¡œ ë‘¡ë‹ˆë‹¤.
    is_checked_by_default = remembered_username != ""
    
    username = st.text_input("ì‚¬ìš©ì ì•„ì´ë””", value=remembered_username)
    remember_id = st.checkbox("ì•„ì´ë”” ì €ì¥", value=is_checked_by_default)
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
    
    if st.button("ë¡œê·¸ì¸"):
        if username in credentials and credentials[username] == password:
            st.session_state.authenticated = True
            st.session_state.username = username
            
            # 'ì•„ì´ë”” ì €ì¥' ì²´í¬ë°•ìŠ¤ ìƒíƒœì— ë”°ë¼ localStorageì— ì €ì¥ ë˜ëŠ” ì‚­ì œ
            if remember_id:
                localS.setItem("remembered_username", username)
            else:
                localS.setItem("remembered_username", "") # ì €ì¥ëœ ì•„ì´ë”” ì‚­ì œ

            # (â˜…â˜…â˜… ìˆ˜ì •ëœ ë¶€ë¶„ â˜…â˜…â˜…)
            # localStorageê°€ ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ ì•„ì£¼ ì§§ì€ ì§€ì—° ì‹œê°„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            time.sleep(0.1)
            
            st.rerun()
        else:
            st.error("ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# --- 2. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (ì¸ì¦ í›„) ---
with st.sidebar:
    st.header(f"í™˜ì˜í•©ë‹ˆë‹¤, {st.session_state.username}ë‹˜!")
    if st.button("ë¡œê·¸ì•„ì›ƒ"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

    def start_new_chat():
        st.session_state.messages = []
        st.session_state.current_chat_file = None
        st.session_state.last_response_text = None # TTSìš© ë§ˆì§€ë§‰ ì‘ë‹µ ì´ˆê¸°í™”

    def auto_save_chat():
        HISTORY_DIR = get_user_history_dir()
        if st.session_state.get("current_chat_file") and st.session_state.get("messages"):
            save_path = HISTORY_DIR / st.session_state.current_chat_file
            # ë©”ì‹œì§€ ë‚´ìš©ì´ ë¦¬ìŠ¤íŠ¸(ì´ë¯¸ì§€ í¬í•¨)ì¸ ê²½ìš° í…ìŠ¤íŠ¸ë§Œ ì €ì¥í•˜ë„ë¡ ì „ì²˜ë¦¬
            messages_to_save = []
            for msg in st.session_state.messages:
                content_to_save = msg["content"]
                if isinstance(content_to_save, list):
                    # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³  ì´ë¯¸ì§€ ì •ë³´ëŠ” ê°„ëµíˆ í‘œì‹œ
                    text_part = next((item["text"] for item in content_to_save if item["type"] == "text"), "")
                    content_to_save = f"{text_part}\n[ì²¨ë¶€ ì´ë¯¸ì§€ í¬í•¨ë¨]"
                messages_to_save.append({"role": msg["role"], "content": content_to_save})
                
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(messages_to_save, f, ensure_ascii=False, indent=2)

    def load_chat(filename: str):
        HISTORY_DIR = get_user_history_dir()
        load_path = HISTORY_DIR / filename
        with open(load_path, "r", encoding="utf-8") as f:
            st.session_state.messages = json.load(f)
        st.session_state.current_chat_file = filename
        st.session_state.last_response_text = None # ì±„íŒ… ë¡œë“œ ì‹œ TTS ì´ˆê¸°í™”

    def delete_chat(filename: str):
        HISTORY_DIR = get_user_history_dir()
        if st.session_state.get("current_chat_file") == filename:
            start_new_chat()
        file_to_delete = HISTORY_DIR / filename
        if file_to_delete.exists():
            file_to_delete.unlink()
            st.toast(f"'{filename}'ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    st.button("ìƒˆë¡œìš´ ì±„íŒ… ì—´ê¸°", on_click=start_new_chat, use_container_width=True)
    st.divider()

    # LLM ê´€ë¦¬ UI
    saved_model = localS.getItem("selected_model")
    saved_category = saved_model[0] if saved_model else ""
    saved_item = saved_model[1] if saved_model else ""
    
    categories = list(llm_options.keys())
    category_index = categories.index(saved_category) if saved_category in categories else 0
    
    st.header("LLM ê´€ë¦¬")
    selected_category = st.selectbox("LLMë¥¼ ì„ íƒí•˜ì„¸ìš”:", categories, index=category_index)
    
    model_options = llm_options[selected_category]
    item_index = model_options.index(saved_item) if saved_item in model_options else 0
    selected_item = st.selectbox(f"{selected_category} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”:", model_options, index=item_index)
    localS.setItem("selected_model", [selected_category,selected_item])

    # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ê²½ê³ 
    is_multimodal = selected_item in ['gpt-4o', 'gpt-4-turbo', 'claude-3-opus-20240229', 'claude-3-5-sonnet-20240620', 'gemini-1.5-pro-latest']
    if not is_multimodal:
        st.info("ğŸ’¡ ì´ë¯¸ì§€ ë¶„ì„/í¸ì§‘ì„ ìœ„í•´ì„œëŠ” 'gpt-4o', 'claude-3-sonnet' ë“± ë©€í‹°ëª¨ë‹¬ ì§€ì› ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.divider()
    st.header("ì´ë¯¸ì§€ ìƒì„± ì„¤ì •")
    image_gen_model = st.selectbox("ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì„ íƒ:", ["DALL-E 3", "Google Nano Banana"], index=0)
    if "image_gen_model" not in st.session_state:
        st.session_state.image_gen_model = "DALL-E 3"
    st.session_state.image_gen_model = image_gen_model


    st.divider()
    st.header(f"MCP ì„œë²„ ê´€ë¦¬ ({st.session_state.username})")
    mcp_config = load_mcp_config()
    with st.expander("ì„œë²„ ëª©ë¡ ë³´ê¸°/ê´€ë¦¬"):
        st.json(mcp_config, expanded=False)
        servers = list(mcp_config["mcpServers"].keys())
        server_to_delete = st.selectbox("ì‚­ì œí•  ì„œë²„ ì„ íƒ", [""] + servers)
        if st.button("ì„ íƒëœ ì„œë²„ ì‚­ì œ", type="primary"):
            if server_to_delete and server_to_delete in mcp_config["mcpServers"]:
                del mcp_config["mcpServers"][server_to_delete]
                save_mcp_config(mcp_config)
                st.success(f"'{server_to_delete}' ì„œë²„ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                time.sleep(1); st.rerun()
        st.markdown("---")
        st.write("**ì„œë²„ ìŠ¤ìœ„ì¹˜**")
        server_configs = mcp_config.get("mcpServers", {})
        config_changed = False
        for server_name, config in server_configs.items():
            is_active = st.toggle(
                server_name,
                value=config.get("active", True),
                key=f"toggle_{server_name}"
            )
            if is_active != config.get("active", True):
                mcp_config["mcpServers"][server_name]["active"] = is_active
                config_changed = True
        if config_changed:
            save_mcp_config(mcp_config)
            st.toast("ì„œë²„ í™œì„±í™” ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.markdown("---")
        st.write("**ìƒˆ ì„œë²„ ì¶”ê°€**")
        new_server_name = st.text_input("ìƒˆ ì„œë²„ ì´ë¦„")
        new_server_config_str = st.text_area("ìƒˆ ì„œë²„ JSON ì„¤ì •", height=200, placeholder='{\n  "description": "...",\n ...}')
        if st.button("ìƒˆ ì„œë²„ ì¶”ê°€"):
            if new_server_name and new_server_config_str:
                try:
                    new_config = json.loads(new_server_config_str)
                    mcp_config["mcpServers"][new_server_name] = new_config
                    save_mcp_config(mcp_config)
                    st.success(f"'{new_server_name}' ì„œë²„ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    time.sleep(1); st.rerun()
                except json.JSONDecodeError: st.error("ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
            else: st.warning("ì„œë²„ ì´ë¦„ê³¼ ì„¤ì •ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()
    st.header("ì €ì¥ëœ ëŒ€í™”")

    # ëŒ€í™” ëª©ë¡ ê´€ë¦¬ UI (ê¸°ì¡´ê³¼ ë™ì¼)
    HISTORY_DIR = get_user_history_dir()
    if "editing_chat_file" not in st.session_state:
        st.session_state.editing_chat_file = None
    if "show_all_chats" not in st.session_state:
        st.session_state.show_all_chats = False
    if "dialog_items_to_show" not in st.session_state:
        st.session_state.dialog_items_to_show = 10

    def display_chat_item(filename: str, key_prefix: str):
        """ëŒ€í™” ëª©ë¡ ì•„ì´í…œì„ í‘œì‹œí•˜ê³  ìˆ˜ì •/ì‚­ì œ UIë¥¼ ì œê³µí•˜ëŠ” í•¨ìˆ˜"""
        is_editing = st.session_state.get("editing_chat_file") == filename

        if is_editing:
            # ì´ë¦„ ìˆ˜ì • ëª¨ë“œ UI
            c1, c2, c3 = st.columns([0.7, 0.15, 0.15])
            with c1:
                new_name_base = st.text_input(
                    "ìƒˆ íŒŒì¼ ì´ë¦„",
                    value=filename.removesuffix(".json"),
                    key=f"text_{key_prefix}_{filename}",
                    label_visibility="collapsed"
                )
            with c2:
                if st.button("ì €ì¥", key=f"save_{key_prefix}_{filename}", use_container_width=True, type="primary"):
                    rename_chat(filename, st.session_state[f"text_{key_prefix}_{filename}"])
                    st.session_state.editing_chat_file = None
                    st.rerun()
            with c3:
                if st.button("ì·¨ì†Œ", key=f"cancel_{key_prefix}_{filename}", use_container_width=True):
                    st.session_state.editing_chat_file = None
                    st.rerun()
        else:
            # ì¼ë°˜ í‘œì‹œ ëª¨ë“œ UI
            c1, c2, c3 = st.columns([0.75, 0.125, 0.125])
            with c1:
                is_active_chat = st.session_state.get("current_chat_file") == filename
                button_type = "primary" if is_active_chat else "secondary"
                if st.button(filename, key=f"load_{key_prefix}_{filename}", use_container_width=True, type=button_type):
                    if not is_active_chat:
                        load_chat(filename)
                        st.session_state.editing_chat_file = None
                        st.rerun()
            with c2:
                if st.button("âœï¸", key=f"edit_{key_prefix}_{filename}", use_container_width=True, help="ì´ë¦„ ë³€ê²½"):
                    st.session_state.editing_chat_file = filename
                    st.rerun()
            with c3:
                if st.button("X", key=f"delete_{key_prefix}_{filename}", use_container_width=True, help=f"{filename} ì‚­ì œ"):
                    delete_chat(filename)
                    st.rerun()

    try:
        saved_chats_paths = [p for p in HISTORY_DIR.glob("*.json")]
        saved_chats_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        saved_chats = [p.name for p in saved_chats_paths]
    except FileNotFoundError:
        saved_chats = []

    @st.dialog("ì „ì²´ ëŒ€í™” ëª©ë¡")
    def show_all_chats_dialog(older_chats_list):
        st.write(f"ì´ {len(saved_chats)}ê°œì˜ ëŒ€í™”ê°€ ìˆìŠµë‹ˆë‹¤.")
        items_to_show_count = st.session_state.get("dialog_items_to_show", 10)
        chats_to_display = older_chats_list[:items_to_show_count]
        for filename in chats_to_display:
            display_chat_item(filename, key_prefix="dialog")
        st.divider()
        if len(older_chats_list) > items_to_show_count:
            if st.button("ë”ë³´ê¸°", use_container_width=True):
                st.session_state.dialog_items_to_show += 10
                st.rerun()
        if st.button("ë‹«ê¸°", use_container_width=True, type="primary"):
            st.session_state.show_all_chats = False
            st.session_state.editing_chat_file = None
            st.rerun()
    
    if not saved_chats:
        st.write("ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        recent_chats = saved_chats[:10]
        older_chats = saved_chats[10:]
        for filename in recent_chats:
            display_chat_item(filename, key_prefix="recent")
        if older_chats:
            if st.button("ë” ë³´ê¸°...", use_container_width=True):
                st.session_state.show_all_chats = True
                st.session_state.dialog_items_to_show = 10
                st.rerun()
    
    if st.session_state.get("show_all_chats"):
        older_chats = saved_chats[10:]
        show_all_chats_dialog(older_chats)


# --- ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_chat_file" not in st.session_state:
    st.session_state.current_chat_file = None
if "last_response_text" not in st.session_state:
    st.session_state.last_response_text = None

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for i, message in enumerate(st.session_state.messages):
    is_last_item = (i == len(st.session_state.messages) - 1)
    with st.chat_message(message["role"]):
        content = message["content"]
        if isinstance(content, str):
            st.markdown(content)
        elif isinstance(content, list):
            # ì´ë¯¸ì§€ í¬í•¨ ë©”ì‹œì§€ ë Œë”ë§
            for item in content:
                if item['type'] == 'text':
                    st.markdown(item['text'])
                elif item['type'] == 'image_url':
                    st.image(item['image_url']['url'], width=300)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ì´ê³  ë§ˆì§€ë§‰ ë©”ì‹œì§€ì¸ ê²½ìš° ë¶€ê°€ ì •ë³´ ë° TTS í‘œì‹œ (ì¤‘ë³µ ë°©ì§€)
        if message["role"] == "assistant" and is_last_item:
            st.badge("Answer by "+selected_item+"", icon=":material/check:", color="green")
            
            # TTS ë²„íŠ¼ ë° í”Œë ˆì´ì–´
            if st.session_state.last_response_text:
                # ì–¸ì–´ ê°ì§€
                resp_lang = detect_language(st.session_state.last_response_text)
                if st.button(f"ğŸ”Š ë‹µë³€ ë“£ê¸° ({resp_lang.upper()})", key=f"tts_btn_{i}"):
                    with st.spinner("ìŒì„±ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        audio_stream = text_to_speech_stream(st.session_state.last_response_text, lang=resp_lang)
                        if audio_stream:
                            st.audio(audio_stream, format="audio/mp3", start_time=0)

st.markdown(
    """
    <style>
    @media(max-width:1024px){
        .stBottom{
        bottom:60px;
        }
    }
    /* ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ìŠ¤íƒ€ì¼ ì¡°ì • */
    .stAudio {
        margin-top: 10px;
        width: 300px !important;
    }
    </style>
    """,unsafe_allow_html=True
)

# --- [ì‹ ê·œ] ì…ë ¥ì°½ ì„¤ì • (ì˜¤ë””ì˜¤ ë° íŒŒì¼ ì²¨ë¶€ í™œì„±í™”) ---
prompt_data = st.chat_input(
    "ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì²¨ë¶€í•˜ì„¸ìš”.",
    accept_audio=True,
    accept_file=True,
    file_type=["txt", "csv", "py", "js", "html", "css", "java", "ts", "pdf", "docx", "png", "jpg", "jpeg", "gif"]
)

if prompt_data:
    if not st.session_state.get("current_chat_file"):
        st.session_state.current_chat_file = generate_filename_with_timestamp()

    user_text = prompt_data.get("text", "")
    uploaded_files = prompt_data.get("files", [])
    audio_data = prompt_data.get("audio")

    # [ì‹ ê·œ] ìŒì„± ì…ë ¥(STT) ì²˜ë¦¬
    if audio_data:
        with st.status("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...") as status:
            try:
                # ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ ë° Whisper API í˜¸ì¶œ
                audio_bytes = None
                audio_format = "audio/wav"

                # 1. ë”•ì…”ë„ˆë¦¬ í˜•íƒœ (data í‚¤)
                if isinstance(audio_data, dict) and "data" in audio_data:
                    audio_bytes = audio_data["data"]
                    audio_format = audio_data.get("format", "audio/wav")
                # 2. BytesIO/UploadedFile í˜•íƒœ (getvalue ë©”ì„œë“œ)
                elif hasattr(audio_data, "getvalue"):
                    audio_bytes = audio_data.getvalue()
                    audio_format = getattr(audio_data, "type", "audio/wav")
                # 3. ì§ì ‘ ë°”ì´íŠ¸ í˜•íƒœ
                elif isinstance(audio_data, bytes):
                    audio_bytes = audio_data

                if audio_bytes:
                    extension = "wav"
                    if "webm" in audio_format: extension = "webm"
                    elif "mp3" in audio_format: extension = "mp3"
                    elif "wav" in audio_format: extension = "wav"
                    elif "ogg" in audio_format: extension = "ogg"

                    audio_file = io.BytesIO(audio_bytes)
                    audio_file.name = f"input.{extension}" 
                    
                    transcript = openai_client.audio.transcriptions.create(
                        model="whisper-1", 
                        file=audio_file,
                        response_format="text"
                    )
                    if transcript and transcript.strip():
                        transcript_text = transcript.strip()
                        user_text = (user_text + " " + transcript_text).strip() if user_text else transcript_text
                        status.update(label=f"ìŒì„± ë³€í™˜ ì™„ë£Œ! ({extension})", state="complete")
                        st.toast(f"ğŸ™ï¸ ì¸ì‹ëœ ë‚´ìš©: {transcript_text}")
                    else:
                        st.warning("ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        status.update(label="ìŒì„± ë³€í™˜ ì‹¤íŒ¨ (í…ìŠ¤íŠ¸ ì—†ìŒ)", state="error")
                else:
                    st.error("ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    status.update(label="ì˜¤ë””ì˜¤ ë°ì´í„° ì˜¤ë¥˜", state="error")
            except Exception as e:
                st.error(f"ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                status.update(label="ìŒì„± ë³€í™˜ ì‹¤íŒ¨", state="error")
                
    # í…ìŠ¤íŠ¸, íŒŒì¼, ë˜ëŠ” ìŒì„± ì „ì‚¬ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
    if not user_text and not uploaded_files and not audio_data:
        st.stop()

    extracted_texts = []
    image_data = None
    
    # íŒŒì¼ ì²˜ë¦¬ ë¡œì§
    if uploaded_files:
        for uploaded_file in uploaded_files:
            if "image" in uploaded_file.type:
                # ì´ë¯¸ì§€ëŠ” í•˜ë‚˜ë§Œ ì²˜ë¦¬ (ë©€í‹° ì´ë¯¸ì§€ ì§€ì› í•„ìš” ì‹œ ìˆ˜ì •)
                if image_data is None:
                    base64_image = encode_image_to_base64(uploaded_file)
                    if base64_image:
                        image_data = base64_image
                        with st.chat_message("user"):
                            st.write(f"ğŸ“· ì´ë¯¸ì§€ ì²¨ë¶€: {uploaded_file.name}")
                            st.image(uploaded_file, width=200)
            else:
                # í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼ ì²˜ë¦¬
                text = extract_text_from_file(uploaded_file)
                extracted_texts.append(f"--- íŒŒì¼ëª…: {uploaded_file.name} ---\n{text}\n--------------------------\n")
                with st.chat_message("user"):
                    st.write(f"ğŸ“„ íŒŒì¼ ì²¨ë¶€: {uploaded_file.name}")

    # ìµœì¢… ì‚¬ìš©ì ë©”ì‹œì§€ ì»¨í…ì¸  êµ¬ì„±
    final_user_content: Union[str, List[Any]] = user_text
    
    if extracted_texts:
        # í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš©ì€ í”„ë¡¬í”„íŠ¸ ë’¤ì— ë¶™ì„
        final_user_content = user_text + "\n\n" + "".join(extracted_texts)
    
    if image_data:
        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° LangChain ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        final_user_content = [
            {"type": "text", "text": user_text or "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."},
            {"type": "image_url", "image_url": {"url": image_data}}
        ]
        # ì´ë¯¸ì§€ê°€ ìˆì„ ë•ŒëŠ” í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°ë³´ë‹¤ ìš°ì„ ì‹œ

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ (ì´ë¯¸ì§€ëŠ” ìœ„ì—ì„œ ì´ë¯¸ í‘œì‹œë¨)
    if isinstance(final_user_content, str):
        st.session_state.messages.append({"role": "user", "content": final_user_content})
        with st.chat_message("user"):
            st.markdown(user_text) # ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ ë³´ì—¬ì¤Œ (íŒŒì¼ ë‚´ìš©ì€ ìƒëµ)
    else:
        # ì´ë¯¸ì§€ í¬í•¨ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        st.session_state.messages.append({"role": "user", "content": final_user_content})
        # ì´ë¯¸ì§€ëŠ” íŒŒì¼ ì²˜ë¦¬ ë£¨í”„ì—ì„œ í‘œì‹œí–ˆìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ë§Œ í‘œì‹œ
        if user_text:
             with st.chat_message("user"):
                st.markdown(user_text)


    # ---------------------------------------------------------
    # [ì‹ ê·œ] ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ ì²˜ë¦¬ (ì²¨ë¶€íŒŒì¼ ì—†ê³  í‚¤ì›Œë“œ ê°ì§€ ì‹œ)
    # ---------------------------------------------------------
    if not uploaded_files and isinstance(final_user_content, str) and ("ì´ë¯¸ì§€ ìƒì„±" in final_user_content or "ê·¸ë ¤ì¤˜" in final_user_content):
        with st.chat_message("assistant"):
            st.write(f"ğŸ¨ {st.session_state.image_gen_model} ëª¨ë¸ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            generated_image_url = generate_image(final_user_content, st.session_state.image_gen_model)
            if generated_image_url:
                st.image(generated_image_url, caption=f"Generated by {st.session_state.image_gen_model}")
                # ì´ë¯¸ì§€ URLì„ í…ìŠ¤íŠ¸ë¡œ ì €ì¥ (ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€ ë¬¸ë²• í™œìš©)
                response_text = f"![ìƒì„±ëœ ì´ë¯¸ì§€]({generated_image_url})\n\nìš”ì²­í•˜ì‹  ì´ë¯¸ì§€ë¥¼ {st.session_state.image_gen_model} ëª¨ë¸ë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
            else:
                response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                st.error(response_text)
            
            st.session_state.messages.append({"role": "assistant", "content": response_text})
            st.session_state.last_response_text = None # ì´ë¯¸ì§€ ìƒì„±ì€ TTS ëŒ€ìƒ ì œì™¸
        auto_save_chat()

    # ---------------------------------------------------------
    # ì¼ë°˜ì ì¸ MCP/LLM ì§ˆì˜ ì²˜ë¦¬
    # ---------------------------------------------------------
    else:
        with st.chat_message("assistant"):
            # LangChain ë©”ì‹œì§€ ê°ì²´ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ì²˜ë¦¬ í¬í•¨)
            history = []
            for m in st.session_state.messages[:-1]:
                if m['role'] == 'user':
                    history.append(HumanMessage(content=m['content']))
                else:
                    # ì´ì „ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ê°€ ì´ë¯¸ì§€ ë§í¬ì¸ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                    history.append(AIMessage(content=m['content']))
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì¶œë ¥
            response_text = st.write_stream(process_query(final_user_content, history))

        # ì‘ë‹µ ì €ì¥ ë° TTSìš© í…ìŠ¤íŠ¸ ìºì‹±
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        st.session_state.last_response_text = response_text
        auto_save_chat()
        st.rerun() # ë©”ì‹œì§€ í‘œì‹œ ë£¨í”„ì—ì„œ ë°°ì§€ì™€ TTS ë²„íŠ¼ì´ ë‚˜ì˜¤ë„ë¡ ë¦¬ëŸ°