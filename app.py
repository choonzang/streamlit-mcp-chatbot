import streamlit as st
import os
import json
import time
import asyncio
from dotenv import load_dotenv
from typing import List, Dict, Any
from datetime import datetime
from pathlib import Path

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain.agents import AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser

from langgraph.prebuilt import create_react_agent

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë¡œë“œ ---
load_dotenv()

# -----------------------------------------------------------------------------
# ì‹¤ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³ , ìœ„ì˜ Mock ê°ì²´ë“¤ì€ ì‚­ì œí•˜ì„¸ìš”.
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
# -----------------------------------------------------------------------------

# --- ìƒìˆ˜ ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
MCP_CONFIG_FILE = "mcp.json"
HISTORY_DIR = Path("chat_histories")
HISTORY_DIR.mkdir(exist_ok=True) # ëŒ€í™” ê¸°ë¡ ì €ì¥ í´ë” ìƒì„±

global selected_category
global selected_item
selected_category = None
selected_item = None
llm_options = {
    "OpenAI":['o4-mini','o3','o3-mini','o1','o1-mini','gpt-4o','gpt-4.1'],
    "Claude":['claude-3-7-sonnet-20250219', 'claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022','claude-3-5-sonnet-20240620','claude-sonnet-4-20250514','claude-opus-4-20250514']
}

# --- í—¬í¼ í•¨ìˆ˜ ---

def run_async(func):
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ Streamlitì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í—¬í¼"""
    return asyncio.run(func)

def generate_filename_with_timestamp(prefix="chat_", extension="json"):
    """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    now = datetime.now()
    timestamp_str = now.strftime("%Y%m%d_%H%M%S")
    if prefix:
        filename = f"{prefix}{timestamp_str}.{extension}"
    else:
        filename = f"{timestamp_str}.{extension}"
    return filename

# @st.cache_resource
def get_llm():
    """LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    if selected_category == 'Claude':
        llm = ChatAnthropic(model=selected_item, temperature=0, max_tokens=4096)
    elif selected_category == 'OpenAI':
        llm = ChatOpenAI(model=selected_item, max_tokens=8000)
    else:
        llm = ChatOpenAI(model="o4-mini", max_tokens=8000)
    return llm

# @st.cache_data
def load_mcp_config():
    """mcp.json ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    with open(MCP_CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_mcp_config(config):
    """MCP ì„œë²„ ì„¤ì •ì„ mcp.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(MCP_CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

# --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
# ... (ê¸°ì¡´ process_query, select_mcp_servers í•¨ìˆ˜ë“¤ì€ ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ìœ ì§€) ...
async def select_mcp_servers(query: str, servers_config: Dict) -> List[str]:
    """ì‚¬ìš©ì ì§ˆì˜ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©í•  MCP ì„œë²„ë¥¼ LLMì„ í†µí•´ ì„ íƒí•©ë‹ˆë‹¤."""
    llm = get_llm()
    
    system_prompt = "You are a helpful assistant that selects the most relevant tools for a given user query."
    prompt_template = """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ê·¸ 'description'ì„ ë³´ê³  ì„ íƒí•´ì£¼ì„¸ìš”.
    ì„ íƒëœ ë„êµ¬ì˜ ì´ë¦„(í‚¤ ê°’)ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ëª©ë¡ìœ¼ë¡œë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. (ì˜ˆ: weather,Home Assistant)
    ë§Œì•½ ì í•©í•œ ë„êµ¬ê°€ ì—†ë‹¤ë©´ 'None'ì´ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”.

    [ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡]
    {tools_description}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_query}

    [ì„ íƒëœ ë„êµ¬ ëª©ë¡]
    """
    
    descriptions = "\n".join([f"- {name}: {config['description']}" for name, config in servers_config.items()])
    
    prompt = ChatPromptTemplate.from_template(prompt_template).format(
        tools_description=descriptions,
        user_query=query
    )
    
    response = await llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=prompt)])
    selected = [s.strip() for s in response.content.split(',') if s.strip() and s.strip().lower() != 'none']
    return selected

async def process_query(query: str, chat_history: List):
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ ì„œë²„ ì„ íƒ, ì—ì´ì „íŠ¸ ìƒì„±, ì§ˆì˜ ì‹¤í–‰ì˜ ì „ì²´ ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    mcp_config = load_mcp_config()["mcpServers"]
    llm = get_llm()

    # 1. MCP ì„œë²„ ë¼ìš°íŒ…
    st.write("`1. MCP ì„œë²„ ë¼ìš°íŒ… ì¤‘...`")
    selected_server_names = await select_mcp_servers(query, mcp_config)

    # ìš”ì²­ 1: ì—°ê²°í•  MCP ì„œë²„ê°€ ì—†ì„ ê²½ìš°, LLMìœ¼ë¡œ ì§ì ‘ ì§ˆì˜
    if not selected_server_names:
        st.info("âœ… ì í•©í•œ ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLMì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤.")
        messages = chat_history + [HumanMessage(content=query)]
        response = await llm.ainvoke(messages)
        return response.content

    # 2. ì„ íƒëœ ì„œë²„ ì²˜ë¦¬
    st.write(f"`2. ì„ íƒëœ ì„œë²„: {', '.join(selected_server_names)}`")
    agents = {}
    
    st.write("`3. ì„ íƒëœ MCP ì„œë²„ ì„¸ì…˜ ë° ë„êµ¬ ë¡œë”© ì¤‘...`")

    for name in selected_server_names:
        config = mcp_config[name]
        tools = []

        try:
            conn_type = config.get("transport")

            if conn_type == "stdio":
                server_params = StdioServerParameters(command=config.get("command"), args=config.get("args", []))
                async with stdio_client(server_params) as (read, write):
                    async with ClientSession(read, write) as session:
                        await session.initialize()
                        tools = await load_mcp_tools(session)
                        if not tools:
                            st.warning(f"âœ… '{name}' ì„œë²„ì— ì—°ê²°í–ˆìœ¼ë‚˜, ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            continue
                        st.success(f"âœ… '{name}' ì„œë²„ ì—°ê²° ë° ë„êµ¬ ë¡œë“œ ì„±ê³µ: `{[tool.name for tool in tools]}`")
                        agent = create_react_agent(llm, tools)
                        agent_input = {"messages": chat_history + [HumanMessage(content=query)]}
                        if len(selected_server_names) == 1:
                            response = await agent.ainvoke(agent_input)
                            return response.get('output', response['messages'][-1].content if 'messages' in response and isinstance(response['messages'][-1], AIMessage) else "ì‘ë‹µ ë‚´ìš© íŒŒì‹± ì‹¤íŒ¨")
                        else:
                            agents[name] = agent
            elif conn_type == "sse":
                url = config.get("url")
                headers = config.get("headers", {})
                async with sse_client(url, headers=headers) as (read, write):
                    async with ClientSession(read, write) as session:
                        await session.initialize()
                        tools = await load_mcp_tools(session)
                        if not tools:
                            st.warning(f"âœ… '{name}' ì„œë²„ì— ì—°ê²°í–ˆìœ¼ë‚˜, ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            continue
                        st.success(f"âœ… '{name}' ì„œë²„ ì—°ê²° ë° ë„êµ¬ ë¡œë“œ ì„±ê³µ: `{[tool.name for tool in tools]}`")
                        agent = create_react_agent(llm, tools)
                        agent_input = {"messages": chat_history + [HumanMessage(content=query)]}
                        if len(selected_server_names) == 1:
                            response = await agent.ainvoke(agent_input)
                            return response.get('output', response['messages'][-1].content if 'messages' in response and isinstance(response['messages'][-1], AIMessage) else "ì‘ë‹µ ë‚´ìš© íŒŒì‹± ì‹¤íŒ¨")
                        else:
                            agents[name] = agent
            else:
                st.warning(f"âš ï¸ '{name}' ì„œë²„ì˜ ì—°ê²° íƒ€ì… ('{conn_type}')ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
        except Exception as e:
            st.error(f"âŒ '{name}' MCP ì„œë²„ ì—°ê²° ë˜ëŠ” ë„êµ¬ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    if not agents:
        return "ì„ íƒëœ ëª¨ë“  ì„œë²„ì— ì—°ê²°í•˜ì§€ ëª»í–ˆê±°ë‚˜, ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤."

    st.write(f"`4. {len(agents)}ê°œ ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ. ì§ˆì˜ ì‹¤í–‰...`")
    parallel_runnable = RunnableParallel(**{name: agent for name, agent in agents.items()})
    
    try:
        all_agent_results = await parallel_runnable.ainvoke(agent_input)
        final_responses = {}
        for name, result in all_agent_results.items():
            if 'output' in result:
                final_responses[name] = result['output']
            elif 'messages' in result and isinstance(result['messages'][-1], AIMessage):
                final_responses[name] = result['messages'][-1].content
            else:
                final_responses[name] = f"[{name}] ì‘ë‹µ ë‚´ìš©ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        history_str = "\n".join([f"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}" for m in chat_history])
        synthesis_prompt_template = """
        ë‹¹ì‹ ì€ ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë§ˆìŠ¤í„° AIì…ë‹ˆë‹¤.
        ì•„ë˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë‚˜ì˜ ì¼ê´€ë˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.
        [ëŒ€í™” ê¸°ë¡]
        {chat_history}
        [ì‚¬ìš©ì í˜„ì¬ ì§ˆë¬¸]
        {original_query}
        [ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ]
        {agent_responses}
        [ì¢…í•©ëœ ìµœì¢… ë‹µë³€]
        """
        synthesis_prompt = ChatPromptTemplate.from_template(synthesis_prompt_template)
        synthesis_chain = synthesis_prompt | llm | StrOutputParser()
        final_answer = await synthesis_chain.ainvoke({
            "chat_history": history_str,
            "original_query": query,
            "agent_responses": json.dumps(final_responses, ensure_ascii=False, indent=2)
        })
        return final_answer
    except Exception as e:
        st.error(f"âŒ ë³‘ë ¬ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ë³‘ë ¬ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# --- Streamlit UI êµ¬ì„± ---

st.set_page_config(page_title="MCP Client on Streamlit", layout="wide")
st.title("ğŸ¤– MCP(Model Context Protocol) í´ë¼ì´ì–¸íŠ¸")

# 1. ì¸ì¦ ì²˜ë¦¬
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if st.button("ë¡œê·¸ì¸"):
        if password == os.getenv("APP_PASSWORD"):
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# 2. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (ì¸ì¦ í›„)
# ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.header("ë©”ë‰´")
    if st.button("ë¡œê·¸ì•„ì›ƒ"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()
    
    # --- ì±„íŒ… ê¸°ë¡ ê´€ë¦¬ í•¨ìˆ˜ (â˜…â˜…â˜…â˜…â˜… ë¡œì§ ë³€ê²½ â˜…â˜…â˜…â˜…â˜…) ---
    def start_new_chat():
        """ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ì—¬ ìƒˆë¡œìš´ ì±„íŒ…ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        st.session_state.messages = []
        st.session_state.current_chat_file = None

    def auto_save_chat():
        """í˜„ì¬ ëŒ€í™”ë¥¼ í™œì„± íŒŒì¼ì— ìë™ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if st.session_state.get("current_chat_file") and st.session_state.get("messages"):
            save_path = HISTORY_DIR / st.session_state.current_chat_file
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)

    def load_chat(filename: str):
        """ì„ íƒí•œ íŒŒì¼ì„ ì½ê³  í™œì„± ì±„íŒ…ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
        load_path = HISTORY_DIR / filename
        with open(load_path, "r", encoding="utf-8") as f:
            st.session_state.messages = json.load(f)
        st.session_state.current_chat_file = filename

    def delete_chat(filename: str):
        """ì±„íŒ… ê¸°ë¡ íŒŒì¼ì„ ì‚­ì œí•˜ê³ , í™œì„± ì„¸ì…˜ì´ì—ˆë‹¤ë©´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if st.session_state.get("current_chat_file") == filename:
            start_new_chat() # í™œì„± ì±„íŒ…ì„ ì‚­ì œí•˜ë©´ í™”ë©´ë„ ì´ˆê¸°í™”
        
        file_to_delete = HISTORY_DIR / filename
        if file_to_delete.exists():
            file_to_delete.unlink()
            st.toast(f"'{filename}'ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    st.button("ìƒˆë¡œìš´ ì±„íŒ… ì—´ê¸°", on_click=start_new_chat, use_container_width=True)
    st.divider()

    # --- LLM ë° MCP ì„œë²„ ê´€ë¦¬ ---
    st.header("LLM ê´€ë¦¬")
    selected_category = st.selectbox("LLMë¥¼ ì„ íƒí•˜ì„¸ìš”:", list(llm_options.keys()))
    if selected_category:
        st.subheader(f"{selected_category} ëª¨ë¸ ì„ íƒ")
        selected_item = st.selectbox(f"{selected_category} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”:", llm_options[selected_category])
    else:
        selected_item = None
    
    st.divider()
    st.header("MCP ì„œë²„ ê´€ë¦¬")
    # ... (ê¸°ì¡´ MCP ì„œë²„ ê´€ë¦¬ ì½”ë“œëŠ” ë³€ê²½ ì—†ìŒ) ...
    mcp_config = load_mcp_config()
    with st.expander("ì„œë²„ ëª©ë¡ ë³´ê¸°/ê´€ë¦¬"):
        st.json(mcp_config)
        servers = list(mcp_config["mcpServers"].keys())
        server_to_delete = st.selectbox("ì‚­ì œí•  ì„œë²„ ì„ íƒ", [""] + servers)
        if st.button("ì„ íƒëœ ì„œë²„ ì‚­ì œ", type="primary"):
            if server_to_delete and server_to_delete in mcp_config["mcpServers"]:
                del mcp_config["mcpServers"][server_to_delete]
                save_mcp_config(mcp_config)
                st.success(f"'{server_to_delete}' ì„œë²„ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                time.sleep(1); st.rerun()
        st.markdown("---")
        st.write("**ìƒˆ ì„œë²„ ì¶”ê°€**")
        new_server_name = st.text_input("ìƒˆ ì„œë²„ ì´ë¦„")
        new_server_config_str = st.text_area("ìƒˆ ì„œë²„ JSON ì„¤ì •", height=200, placeholder='{\n  "description": "...",\n ...}')
        if st.button("ìƒˆ ì„œë²„ ì¶”ê°€"):
            if new_server_name and new_server_config_str:
                try:
                    new_config = json.loads(new_server_config_str)
                    mcp_config["mcpServers"][new_server_name] = new_config
                    save_mcp_config(mcp_config)
                    st.success(f"'{new_server_name}' ì„œë²„ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    time.sleep(1); st.rerun()
                except json.JSONDecodeError: st.error("ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
            else: st.warning("ì„œë²„ ì´ë¦„ê³¼ ì„¤ì •ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()

    # --- ì €ì¥ëœ ëŒ€í™” ëª©ë¡ í‘œì‹œ ---
    st.header("ì €ì¥ëœ ëŒ€í™”")
    saved_chats = sorted([f for f in os.listdir(HISTORY_DIR) if f.endswith(".json")], reverse=True)
    
    if not saved_chats:
        st.write("ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    for filename in saved_chats:
        col1, col2 = st.columns([0.85, 0.15])
        with col1:
            # í˜„ì¬ í™œì„± ì±„íŒ…ì€ ë‹¤ë¥´ê²Œ í‘œì‹œ (ì˜ˆ: ë³¼ë“œì²´)
            is_active_chat = st.session_state.get("current_chat_file") == filename
            button_label = f"**{filename}**" if is_active_chat else filename
            if st.button(button_label, key=f"load_{filename}", use_container_width=True):
                load_chat(filename)
                st.rerun()
        with col2:
            if st.button("X", key=f"delete_{filename}", use_container_width=True, help=f"{filename} ì‚­ì œ"):
                delete_chat(filename)
                st.rerun()


# --- ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---

# ì±„íŒ… ê¸°ë¡ ë° í™œì„± íŒŒì¼ ì´ˆê¸°í™” (â˜…â˜…â˜…â˜…â˜… ë¡œì§ ë³€ê²½ â˜…â˜…â˜…â˜…â˜…)
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_chat_file" not in st.session_state:
    st.session_state.current_chat_file = None

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (â˜…â˜…â˜…â˜…â˜… ë¡œì§ ë³€ê²½ â˜…â˜…â˜…â˜…â˜…)
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜ ê·¸ë¦¬ê³  ê±°ì‹¤ ë¶ˆ ì¼œì¤˜)"):
    # 1. ìƒˆ ì±„íŒ…ì¸ ê²½ìš°, í™œì„± íŒŒì¼ ì´ë¦„ ìƒì„±
    if not st.session_state.get("current_chat_file"):
        st.session_state.current_chat_file = generate_filename_with_timestamp()

    # 2. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 3. ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì²˜ë¦¬ ë° í‘œì‹œ
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            history = [
                HumanMessage(content=m['content']) if m['role'] == 'user' else AIMessage(content=m['content'])
                for m in st.session_state.messages[:-1]
            ]
            response = run_async(process_query(prompt, history))
            st.markdown(response)
        st.badge("Answer by "+selected_item+"", icon=":material/check:", color="green")
    
    # 4. ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})
    
    # 5. ëŒ€í™” í„´ì´ ëë‚œ í›„, ì „ì²´ ëŒ€í™”ë¥¼ í™œì„± íŒŒì¼ì— ìë™ ì €ì¥
    auto_save_chat()

    # 6. í™”ë©´ ìƒˆë¡œê³ ì¹¨ (ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì—…ë°ì´íŠ¸ ë“±)
    #st.rerun()